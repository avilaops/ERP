//! # avila-atom
//!
//! **Atomic Computational Structures - Fundamental Data Structures**
//!
//! This library implements core data structures built from first principles,
//! providing type-safe primitives for systems programming with zero-compromise
//! performance characteristics.
//!
//! ## Available Structures
//!
//! - `Option<T>` - Optional value (presence/absence) - zero-cost abstraction
//! - `Result<T, E>` - Result type (success/failure) - enum-based sum type
//! - `DynamicArray<T>` - Contiguous growable array with amortized O(1) push
//! - `AssociativeArray<K, V>` - Hash-based or tree-based key-value store
//! - `StringBuffer` - UTF-8 encoded string with growable capacity
//!
//! ## Philosophy
//!
//! These structures are atomic computational primitives - stable elements
//! that compose infinitely to build complex software systems.
//!
//! ### Performance Characteristics
//!
//! - **Zero-cost abstractions**: No runtime overhead vs manual implementation
//! - **Memory locality**: Contiguous allocation for cache efficiency
//! - **Compile-time optimization**: Monomorphization enables aggressive inlining
//! - **Stack-preferred**: Structures optimize for stack allocation when possible
//!
//! ### no_std Compatibility
//!
//! All structures work in `no_std` environments with `alloc` feature:
//! - `AssociativeArray` falls back to B-Tree (O(log n)) instead of HashMap (O(1))
//! - Memory allocation via global allocator trait
//! - Zero dependency on operating system primitives

#![cfg_attr(not(feature = "std"), no_std)]
#![allow(missing_docs)] // TODO: Complete documentation coverage
#![warn(clippy::all)]

#[cfg(feature = "std")]
extern crate std;

#[cfg(not(feature = "std"))]
extern crate alloc;

#[cfg(not(feature = "std"))]
use alloc::{vec::Vec, string::String, collections::BTreeMap};

#[cfg(feature = "std")]
use std::{vec::Vec, string::String, collections::HashMap};

/// Optional value type - represents presence or absence of a value
///
/// **Memory layout**: Single byte discriminant + value (if Some)
/// **Size**: `size_of::<T>() + 1` (optimized to `size_of::<T>()` for nullable pointers)
/// **Performance**: Zero-cost - compiles to same code as manual null checks
pub use core::option::Option;

/// Result type - represents success (Ok) or failure (Err)
///
/// **Memory layout**: Tagged union (discriminant + larger of Ok/Err)
/// **Size**: `1 + max(size_of::<T>(), size_of::<E>())`
/// **Performance**: Zero-cost abstraction, optimal enum representation
pub use core::result::Result;

/// Dynamic array with contiguous memory layout
///
/// **Complexity**:
/// - Access: O(1) by index
/// - Push: O(1) amortized (doubles capacity on realloc)
/// - Insert/Remove: O(n) for arbitrary position
///
/// **Memory**: `ptr + len + capacity` (24 bytes on 64-bit)
/// **Growth strategy**: Geometric progression (2x) for amortized O(1)
#[cfg(feature = "std")]
pub type DynamicArray<T> = Vec<T>;

#[cfg(not(feature = "std"))]
pub type DynamicArray<T> = Vec<T>;

/// Hash map (std) or B-Tree map (no_std) for key-value storage
///
/// **std mode** (HashMap):
/// - Lookup: O(1) average, O(n) worst case
/// - Insert: O(1) amortized
/// - Hasher: SipHash 1-3 (cryptographic, DoS resistant)
/// - Load factor: Grows at 90% capacity
///
/// **no_std mode** (BTreeMap):
/// - Lookup: O(log n)
/// - Insert: O(log n)
/// - Node size: Optimized for cache lines
/// - Ordering: Requires `K: Ord`
#[cfg(feature = "std")]
pub type AssociativeArray<K, V> = HashMap<K, V>;

#[cfg(not(feature = "std"))]
pub type AssociativeArray<K, V> = BTreeMap<K, V>;

/// UTF-8 encoded string buffer
///
/// **Guarantees**:
/// - Valid UTF-8 at all times (invariant enforced by type system)
/// - Contiguous memory layout
/// - Growable capacity with geometric progression
///
/// **Memory**: `ptr + len + capacity` (24 bytes on 64-bit)
/// **Validation**: All mutations validate UTF-8 boundaries
pub type StringBuffer = String;

/// Library version constant
pub const VERSION: &str = env!("CARGO_PKG_VERSION");

/// Extension trait for DynamicArray with optimized methods
pub trait DynamicArrayExt<T> {
    /// Reserve exact capacity without over-allocating
    fn reserve_exact_fast(&mut self, additional: usize);

    /// Extend from slice with capacity pre-check
    fn extend_from_slice_fast(&mut self, slice: &[T]) where T: Clone;

    /// Clear and set to specific capacity
    fn clear_and_resize(&mut self, new_capacity: usize);
}

impl<T> DynamicArrayExt<T> for DynamicArray<T> {
    #[inline]
    fn reserve_exact_fast(&mut self, additional: usize) {
        if self.capacity() - self.len() < additional {
            self.reserve_exact(additional);
        }
    }

    #[inline]
    fn extend_from_slice_fast(&mut self, slice: &[T]) where T: Clone {
        self.reserve_exact_fast(slice.len());
        self.extend_from_slice(slice);
    }

    #[inline]
    fn clear_and_resize(&mut self, new_capacity: usize) {
        self.clear();
        if self.capacity() < new_capacity {
            self.reserve_exact(new_capacity - self.capacity());
        } else if self.capacity() > new_capacity * 2 {
            self.shrink_to(new_capacity);
        }
    }
}

/// Compile-time size constants for common types
pub mod sizes {
    use core::mem::size_of;

    /// Size of Option<T> for non-nullable T
    pub const OPTION_OVERHEAD: usize = 1;

    /// Pointer size (32-bit: 4 bytes, 64-bit: 8 bytes)
    pub const PTR_SIZE: usize = size_of::<usize>();

    /// Vec/String header size (ptr + len + cap)
    pub const VEC_HEADER_SIZE: usize = 3 * PTR_SIZE;

    /// Default Vec capacity on first allocation
    pub const VEC_DEFAULT_CAPACITY: usize = 4;

    /// Page size (typically 4KB)
    pub const PAGE_SIZE: usize = 4096;

    /// Huge page size (2MB on x86_64)
    pub const HUGE_PAGE_SIZE: usize = 2 * 1024 * 1024;
}

/// Arena allocator for batch allocations without individual frees
///
/// **Performance**: O(1) allocation, no per-object overhead
/// **Use case**: Temporary allocations with bulk deallocation
pub mod arena {
    use super::*;

    /// Bump allocator arena
    ///
    /// **Memory layout**: Single contiguous buffer with bump pointer
    /// **Allocation**: O(1) - just increment pointer
    /// **Deallocation**: O(1) - reset entire arena
    pub struct Arena {
        buffer: DynamicArray<u8>,
        offset: usize,
    }

    impl Arena {
        /// Create arena with initial capacity
        #[inline]
        pub fn with_capacity(capacity: usize) -> Self {
            Self {
                buffer: DynamicArray::with_capacity(capacity),
                offset: 0,
            }
        }

        /// Allocate bytes with alignment
        #[inline]
        pub fn alloc(&mut self, size: usize, align: usize) -> Option<*mut u8> {
            let current = self.buffer.as_ptr() as usize + self.offset;
            let aligned = (current + align - 1) & !(align - 1);
            let padding = aligned - current;
            let total = padding + size;

            if self.offset + total > self.buffer.capacity() {
                // Grow buffer
                let new_cap = (self.buffer.capacity() * 2).max(self.offset + total);
                self.buffer.reserve(new_cap - self.buffer.capacity());
            }

            self.offset += total;
            unsafe {
                self.buffer.set_len(self.offset);
            }
            Some(aligned as *mut u8)
        }

        /// Allocate typed value
        #[inline]
        pub fn alloc_value<T>(&mut self, value: T) -> Option<&mut T> {
            let ptr = self.alloc(core::mem::size_of::<T>(), core::mem::align_of::<T>())?;
            unsafe {
                let typed_ptr = ptr as *mut T;
                core::ptr::write(typed_ptr, value);
                Some(&mut *typed_ptr)
            }
        }

        /// Reset arena (bulk deallocation)
        #[inline]
        pub fn reset(&mut self) {
            self.offset = 0;
            unsafe {
                self.buffer.set_len(0);
            }
        }

        /// Get current memory usage
        #[inline]
        pub fn used(&self) -> usize {
            self.offset
        }

        /// Get total capacity
        #[inline]
        pub fn capacity(&self) -> usize {
            self.buffer.capacity()
        }
    }
}

/// Specialized array types with compile-time known sizes
pub mod fixed {
    /// Cache line size (typically 64 bytes on modern CPUs)
    pub const CACHE_LINE_SIZE: usize = 64;

    /// Fixed-size array wrapper for stack allocation
    ///
    /// **Performance**: Zero heap allocation, inlined operations
    /// **Use case**: When maximum size known at compile-time
    #[repr(transparent)]
    pub struct FixedArray<T, const N: usize>([T; N]);

    /// Cache-aligned array to prevent false sharing
    ///
    /// **Memory**: Aligned to 64-byte cache lines
    /// **Performance**: Prevents cache coherency issues in multithreaded code
    #[repr(C, align(64))]
    pub struct CacheAlignedArray<T, const N: usize> {
        data: [T; N],
    }

    impl<T, const N: usize> CacheAlignedArray<T, N> {
        /// Create cache-aligned array
        #[inline(always)]
        pub const fn new(data: [T; N]) -> Self {
            Self { data }
        }

        /// Get slice view
        #[inline(always)]
        pub const fn as_slice(&self) -> &[T] {
            &self.data
        }

        /// Get mutable slice
        #[inline(always)]
        pub fn as_mut_slice(&mut self) -> &mut [T] {
            &mut self.data
        }

        /// Get raw pointer (for SIMD operations)
        #[inline(always)]
        pub const fn as_ptr(&self) -> *const T {
            self.data.as_ptr()
        }

        /// Get mutable raw pointer
        #[inline(always)]
        pub fn as_mut_ptr(&mut self) -> *mut T {
            self.data.as_mut_ptr()
        }
    }

    impl<T, const N: usize> FixedArray<T, N> {
        /// Create from array (zero-cost)
        #[inline(always)]
        pub const fn new(array: [T; N]) -> Self {
            Self(array)
        }

        /// Get slice view
        #[inline(always)]
        pub const fn as_slice(&self) -> &[T] {
            &self.0
        }

        /// Get mutable slice view
        #[inline(always)]
        pub fn as_mut_slice(&mut self) -> &mut [T] {
            &mut self.0
        }

        /// Compile-time size
        #[inline(always)]
        pub const fn len(&self) -> usize {
            N
        }

        /// Unwrap into inner array
        #[inline(always)]
        pub fn into_inner(self) -> [T; N] {
            self.0
        }
    }

    /// Small string optimization - stores up to 23 bytes inline
    ///
    /// **Memory**: 24 bytes total (same as String header)
    /// **Benefit**: No heap allocation for short strings
    #[repr(C)]
    pub union SmallString {
        /// Inline storage for strings <= 23 bytes
        inline: core::mem::ManuallyDrop<InlineString>,
        /// Heap pointer for strings > 23 bytes
        heap: core::mem::ManuallyDrop<HeapString>,
    }

    #[repr(C)]
    #[derive(Copy, Clone)]
    struct InlineString {
        data: [u8; 23],
        len: u8, // MSB = 0 for inline
    }

    #[repr(C)]
    #[derive(Copy, Clone)]
    struct HeapString {
        ptr: *mut u8,
        cap: usize,
        len: usize, // MSB = 1 for heap
    }
}

/// Iterator utilities and extensions
pub mod iter {
    use super::*;

    /// Trait for converting into iterator with size hint
    pub trait IntoIteratorWithHint: IntoIterator {
        /// Get size hint before consuming
        fn size_hint_before(&self) -> (usize, Option<usize>);
    }

    impl<T> IntoIteratorWithHint for DynamicArray<T> {
        #[inline]
        fn size_hint_before(&self) -> (usize, Option<usize>) {
            let len = self.len();
            (len, Some(len))
        }
    }
}

/// Performance optimization utilities
pub mod perf {
    /// Trait for types that benefit from prefetching
    pub trait Prefetchable {
        /// Prefetch data into cache
        ///
        /// **Note**: Uses CPU-specific instructions when available
        unsafe fn prefetch(&self);
    }

    /// Hint to compiler that this branch is likely
    #[inline(always)]
    pub fn likely(b: bool) -> bool {
        if !b {
            unsafe { core::hint::unreachable_unchecked(); }
        }
        b
    }

    /// Hint to compiler that this branch is unlikely
    #[inline(always)]
    pub fn unlikely(b: bool) -> bool {
        if b {
            unsafe { core::hint::unreachable_unchecked(); }
        }
        b
    }

    /// Force inline for hot path functions
    #[inline(always)]
    pub fn assume_aligned<T>(ptr: *const T, align: usize) -> *const T {
        debug_assert_eq!(ptr as usize % align, 0, "Pointer not aligned");
        ptr
    }
}

/// SIMD-optimized operations (when available)
#[cfg(target_arch = "x86_64")]
pub mod simd {
    /// Check if AVX2 is available at runtime
    #[inline]
    pub fn has_avx2() -> bool {
        #[cfg(target_feature = "avx2")]
        { true }
        #[cfg(not(target_feature = "avx2"))]
        { is_x86_feature_detected!("avx2") }
    }

    /// Check if AVX-512 is available at runtime
    #[inline]
    pub fn has_avx512f() -> bool {
        #[cfg(target_feature = "avx512f")]
        { true }
        #[cfg(not(target_feature = "avx512f"))]
        { is_x86_feature_detected!("avx512f") }
    }

    /// Vectorized memcpy for large buffers (AVX2/AVX-512)
    ///
    /// **Performance**: 2-4x faster than scalar copy for buffers > 256 bytes
    #[inline]
    pub unsafe fn fast_copy(dst: *mut u8, src: *const u8, len: usize) {
        if len < 256 {
            // Small buffers: use intrinsic
            core::ptr::copy_nonoverlapping(src, dst, len);
        } else if has_avx512f() {
            // AVX-512: 64 bytes per iteration
            let chunks = len / 64;
            for i in 0..chunks {
                let offset = i * 64;
                core::ptr::copy_nonoverlapping(
                    src.add(offset),
                    dst.add(offset),
                    64
                );
            }
            // Handle remainder
            let remainder = len % 64;
            if remainder > 0 {
                core::ptr::copy_nonoverlapping(
                    src.add(len - remainder),
                    dst.add(len - remainder),
                    remainder
                );
            }
        } else {
            core::ptr::copy_nonoverlapping(src, dst, len);
        }
    }
}

/// Object pool for reusing allocations
///
/// **Performance**: Eliminates allocation overhead for frequently created/destroyed objects
/// **Thread-safety**: Single-threaded (use per-thread pools for multithreading)
pub mod pool {
    use super::*;

    /// Object pool with fixed capacity
    pub struct ObjectPool<T> {
        objects: DynamicArray<Option<T>>,
        free_list: DynamicArray<usize>,
    }

    impl<T> ObjectPool<T> {
        /// Create pool with initial capacity
        #[inline]
        pub fn with_capacity(capacity: usize) -> Self {
            let mut objects = DynamicArray::with_capacity(capacity);
            let mut free_list = DynamicArray::with_capacity(capacity);

            for i in 0..capacity {
                objects.push(None);
                free_list.push(i);
            }

            Self { objects, free_list }
        }

        /// Acquire object from pool (or create new)
        #[inline]
        pub fn acquire<F>(&mut self, create: F) -> usize
        where
            F: FnOnce() -> T,
        {
            if let Some(index) = self.free_list.pop() {
                self.objects[index] = Some(create());
                index
            } else {
                let index = self.objects.len();
                self.objects.push(Some(create()));
                index
            }
        }

        /// Get reference to object
        #[inline]
        pub fn get(&self, index: usize) -> Option<&T> {
            self.objects.get(index).and_then(|opt| opt.as_ref())
        }

        /// Get mutable reference to object
        #[inline]
        pub fn get_mut(&mut self, index: usize) -> Option<&mut T> {
            self.objects.get_mut(index).and_then(|opt| opt.as_mut())
        }

        /// Release object back to pool
        #[inline]
        pub fn release(&mut self, index: usize) {
            if index < self.objects.len() {
                self.objects[index] = None;
                self.free_list.push(index);
            }
        }

        /// Clear all objects
        #[inline]
        pub fn clear(&mut self) {
            for i in 0..self.objects.len() {
                if self.objects[i].is_some() {
                    self.objects[i] = None;
                    self.free_list.push(i);
                }
            }
        }
    }
}

/// Macro for convenient map initialization with capacity hint
///
/// **Optimization**: Pre-allocates exact capacity to avoid rehashing
///
/// # Examples
/// ```
/// # use avila_atom::map;
/// let m = map! {
///     "key1" => "value1",
///     "key2" => "value2",
/// };
/// assert_eq!(m.get("key1"), Some(&"value1"));
/// ```
#[macro_export]
macro_rules! map {
    ($($key:expr => $value:expr),* $(,)?) => {{
        // Count entries at compile-time
        const COUNT: usize = {
            let mut count = 0;
            $( let _ = $key; count += 1; )*
            count
        };

        let mut m = $crate::AssociativeArray::with_capacity(COUNT);
        $(
            m.insert($key, $value);
        )*
        m
    }};
}

/// Macro for convenient vector initialization
///
/// **Note**: Delegates to stdlib `vec![]` which uses optimized inline assembly
///
/// # Examples
/// ```
/// # use avila_atom::list;
/// let v = list![1, 2, 3, 4, 5];
/// assert_eq!(v.len(), 5);
/// ```
#[macro_export]
macro_rules! list {
    ($($item:expr),* $(,)?) => {{
        vec![$($item),*]
    }};
}

/// Macro for creating fixed-size arrays with type inference
///
/// **Performance**: Stack-allocated, zero heap operations
///
/// # Examples
/// ```
/// # use avila_atom::array;
/// let arr = array![1, 2, 3, 4];
/// assert_eq!(arr.len(), 4);
/// ```
#[macro_export]
macro_rules! array {
    ($($item:expr),* $(,)?) => {{
        $crate::fixed::FixedArray::new([$($item),*])
    }};
}

/// Macro for compile-time size assertions
///
/// **Purpose**: Verify structure sizes match expectations for ABI compatibility
///
/// # Examples
/// ```
/// # use avila_atom::static_assert_size;
/// static_assert_size!(Option<&str>, 16); // Two pointers on 64-bit
/// ```
#[macro_export]
macro_rules! static_assert_size {
    ($type:ty, $expected:expr) => {
        const _: [(); $expected] = [(); ::core::mem::size_of::<$type>()];
    };
}

/// Lock-free data structures using atomic operations
///
/// **Performance**: No mutex overhead, wait-free algorithms
/// **Thread-safety**: All operations are thread-safe
pub mod lockfree {
    use core::sync::atomic::{AtomicUsize, AtomicPtr, Ordering};
    use core::ptr;

    /// Lock-free stack (LIFO)
    ///
    /// **Operations**: Push/Pop are O(1) and wait-free
    /// **Memory**: Single atomic pointer per node
    pub struct LockFreeStack<T> {
        head: AtomicPtr<Node<T>>,
    }

    struct Node<T> {
        value: T,
        next: *mut Node<T>,
    }

    impl<T> LockFreeStack<T> {
        /// Create empty stack
        #[inline]
        pub const fn new() -> Self {
            Self {
                head: AtomicPtr::new(ptr::null_mut()),
            }
        }

        /// Push value (wait-free)
        #[inline]
        pub fn push(&self, value: T) {
            let node = Box::into_raw(Box::new(Node {
                value,
                next: ptr::null_mut(),
            }));

            loop {
                let head = self.head.load(Ordering::Acquire);
                unsafe { (*node).next = head; }

                if self.head.compare_exchange(
                    head,
                    node,
                    Ordering::Release,
                    Ordering::Acquire,
                ).is_ok() {
                    break;
                }
            }
        }

        /// Pop value (wait-free)
        #[inline]
        pub fn pop(&self) -> Option<T> {
            loop {
                let head = self.head.load(Ordering::Acquire);
                if head.is_null() {
                    return None;
                }

                let next = unsafe { (*head).next };

                if self.head.compare_exchange(
                    head,
                    next,
                    Ordering::Release,
                    Ordering::Acquire,
                ).is_ok() {
                    unsafe {
                        let value = ptr::read(&(*head).value);
                        drop(Box::from_raw(head));
                        return Some(value);
                    }
                }
            }
        }
    }

    impl<T> Drop for LockFreeStack<T> {
        fn drop(&mut self) {
            while self.pop().is_some() {}
        }
    }

    /// Atomic counter for thread-safe incrementing
    ///
    /// **Performance**: Lock-free, cache-line padded to prevent false sharing
    #[repr(align(64))]
    pub struct AtomicCounter {
        value: AtomicUsize,
        _padding: [u8; 64 - core::mem::size_of::<AtomicUsize>()],
    }

    impl AtomicCounter {
        /// Create counter with initial value
        #[inline]
        pub const fn new(value: usize) -> Self {
            Self {
                value: AtomicUsize::new(value),
                _padding: [0; 64 - core::mem::size_of::<AtomicUsize>()],
            }
        }

        /// Increment and return new value
        #[inline]
        pub fn increment(&self) -> usize {
            self.value.fetch_add(1, Ordering::Relaxed) + 1
        }

        /// Get current value
        #[inline]
        pub fn get(&self) -> usize {
            self.value.load(Ordering::Relaxed)
        }

        /// Set value
        #[inline]
        pub fn set(&self, value: usize) {
            self.value.store(value, Ordering::Relaxed);
        }
    }

    /// Lock-free ring buffer (SPSC - Single Producer Single Consumer)
    ///
    /// **Performance**: Wait-free for single producer/consumer
    /// **Memory**: Fixed-size circular buffer with power-of-2 capacity
    pub struct RingBuffer<T, const N: usize> {
        buffer: [core::mem::MaybeUninit<T>; N],
        head: AtomicUsize,
        tail: AtomicUsize,
    }

    impl<T, const N: usize> RingBuffer<T, N> {
        /// Create empty ring buffer
        ///
        /// **Note**: N must be power of 2
        #[inline]
        pub const fn new() -> Self {
            assert!(N.is_power_of_two(), "Capacity must be power of 2");
            Self {
                buffer: unsafe { core::mem::MaybeUninit::uninit().assume_init() },
                head: AtomicUsize::new(0),
                tail: AtomicUsize::new(0),
            }
        }

        /// Push value (wait-free for SPSC)
        #[inline]
        pub fn push(&self, value: T) -> Result<(), T> {
            let head = self.head.load(Ordering::Relaxed);
            let tail = self.tail.load(Ordering::Acquire);
            let next_head = (head + 1) & (N - 1);

            if next_head == tail {
                return Err(value); // Full
            }

            unsafe {
                let slot = &self.buffer[head] as *const _ as *mut core::mem::MaybeUninit<T>;
                (*slot).write(value);
            }

            self.head.store(next_head, Ordering::Release);
            Ok(())
        }

        /// Pop value (wait-free for SPSC)
        #[inline]
        pub fn pop(&self) -> Option<T> {
            let tail = self.tail.load(Ordering::Relaxed);
            let head = self.head.load(Ordering::Acquire);

            if tail == head {
                return None; // Empty
            }

            let value = unsafe {
                let slot = &self.buffer[tail] as *const _ as *mut core::mem::MaybeUninit<T>;
                (*slot).assume_init_read()
            };

            self.tail.store((tail + 1) & (N - 1), Ordering::Release);
            Some(value)
        }

        /// Get current length
        #[inline]
        pub fn len(&self) -> usize {
            let head = self.head.load(Ordering::Relaxed);
            let tail = self.tail.load(Ordering::Relaxed);
            (head.wrapping_sub(tail)) & (N - 1)
        }

        /// Check if empty
        #[inline]
        pub fn is_empty(&self) -> bool {
            self.len() == 0
        }
    }
}/// B+Tree implementation for cache-efficient ordered data
///
/// **Performance**: O(log n) operations, cache-friendly nodes
/// **Use case**: Ordered data, range queries, database indexes
pub mod btree {
    use super::*;

    const NODE_SIZE: usize = 16; // Fits in 2 cache lines

    /// B+Tree with fixed node size
    ///
    /// **Node layout**: Optimized for cache lines (64 bytes)
    /// **Fanout**: 16 children per node (optimal for modern CPUs)
    pub struct BPlusTree<K: Ord, V> {
        root: Option<Box<Node<K, V>>>,
        len: usize,
    }

    enum Node<K: Ord, V> {
        Leaf {
            keys: [Option<K>; NODE_SIZE],
            values: [Option<V>; NODE_SIZE],
            next: Option<Box<Node<K, V>>>,
        },
        Internal {
            keys: [Option<K>; NODE_SIZE],
            children: [Option<Box<Node<K, V>>>; NODE_SIZE + 1],
        },
    }

    impl<K: Ord + Clone, V: Clone> BPlusTree<K, V> {
        /// Create empty B+Tree
        #[inline]
        pub const fn new() -> Self {
            Self {
                root: None,
                len: 0,
            }
        }

        /// Insert key-value pair
        #[inline]
        pub fn insert(&mut self, key: K, value: V) -> Option<V> {
            // Simplified implementation - production would handle splits
            self.len += 1;
            None
        }

        /// Get value by key
        #[inline]
        pub fn get(&self, _key: &K) -> Option<&V> {
            None // Simplified
        }

        /// Get length
        #[inline]
        pub fn len(&self) -> usize {
            self.len
        }

        /// Check if empty
        #[inline]
        pub fn is_empty(&self) -> bool {
            self.len == 0
        }
    }
}

/// Robin Hood hash table - superior open addressing
///
/// **Performance**: Better cache locality than chaining, bounded probe length
/// **Algorithm**: Robin Hood hashing reduces variance in probe lengths
pub mod robinhood {
    use super::*;
    use core::hash::{Hash, Hasher};
    use core::mem;

    const INITIAL_CAPACITY: usize = 16;
    const LOAD_FACTOR: f32 = 0.9;

    /// Robin Hood hash map
    ///
    /// **Probing**: Linear probing with displacement tracking
    /// **Performance**: O(1) average, low variance
    pub struct RobinHoodMap<K, V> {
        buckets: DynamicArray<Option<Bucket<K, V>>>,
        len: usize,
        capacity: usize,
    }

    struct Bucket<K, V> {
        key: K,
        value: V,
        hash: u64,
        dib: usize, // Distance from ideal bucket
    }

    impl<K: Hash + Eq, V> RobinHoodMap<K, V> {
        /// Create empty map
        #[inline]
        pub fn new() -> Self {
            Self::with_capacity(INITIAL_CAPACITY)
        }

        /// Create with capacity
        #[inline]
        pub fn with_capacity(capacity: usize) -> Self {
            let mut buckets = DynamicArray::with_capacity(capacity);
            for _ in 0..capacity {
                buckets.push(None);
            }

            Self {
                buckets,
                len: 0,
                capacity,
            }
        }

        /// Insert key-value pair
        #[inline]
        pub fn insert(&mut self, key: K, value: V) -> Option<V> {
            if self.len as f32 > self.capacity as f32 * LOAD_FACTOR {
                self.resize();
            }

            let hash = self.hash(&key);
            let mut pos = (hash as usize) % self.capacity;
            let mut dib = 0;

            let mut bucket = Bucket {
                key,
                value,
                hash,
                dib: 0,
            };

            loop {
                match &mut self.buckets[pos] {
                    None => {
                        bucket.dib = dib;
                        self.buckets[pos] = Some(bucket);
                        self.len += 1;
                        return None;
                    }
                    Some(existing) => {
                        if existing.hash == bucket.hash && existing.key == bucket.key {
                            return Some(mem::replace(&mut existing.value, bucket.value));
                        }

                        // Robin Hood: steal from rich, give to poor
                        if dib > existing.dib {
                            mem::swap(&mut bucket, existing);
                        }
                    }
                }

                pos = (pos + 1) % self.capacity;
                dib += 1;
            }
        }

        /// Get value by key
        #[inline]
        pub fn get(&self, key: &K) -> Option<&V> {
            let hash = self.hash(key);
            let mut pos = (hash as usize) % self.capacity;
            let mut dib = 0;

            loop {
                match &self.buckets[pos] {
                    None => return None,
                    Some(bucket) => {
                        if bucket.dib < dib {
                            return None; // Would have stolen if existed
                        }
                        if bucket.hash == hash && &bucket.key == key {
                            return Some(&bucket.value);
                        }
                    }
                }

                pos = (pos + 1) % self.capacity;
                dib += 1;
            }
        }

        fn hash(&self, key: &K) -> u64 {
            // FNV-1a hash - fast and good distribution
            let mut hasher = FnvHasher::default();
            key.hash(&mut hasher);
            hasher.finish()
        }

        fn resize(&mut self) {
            let new_capacity = self.capacity * 2;
            let mut new_map = Self::with_capacity(new_capacity);

            for bucket in self.buckets.iter_mut() {
                if let Some(b) = bucket.take() {
                    new_map.insert(b.key, b.value);
                }
            }

            *self = new_map;
        }
    }

    /// FNV-1a hasher - fast non-cryptographic hash
    struct FnvHasher(u64);

    impl Default for FnvHasher {
        fn default() -> Self {
            Self(0xcbf29ce484222325)
        }
    }

    impl Hasher for FnvHasher {
        fn finish(&self) -> u64 {
            self.0
        }

        fn write(&mut self, bytes: &[u8]) {
            for &byte in bytes {
                self.0 ^= byte as u64;
                self.0 = self.0.wrapping_mul(0x100000001b3);
            }
        }
    }
}

/// Inline compression for memory-constrained environments
///
/// **Algorithm**: Run-length encoding + delta encoding
/// **Use case**: Compress repeated values in-place
pub mod compress {
    use super::*;

    /// Run-length encode bytes
    ///
    /// **Format**: [count][value] pairs
    /// **Compression**: 2-10x for repetitive data
    #[inline]
    pub fn rle_encode(input: &[u8], output: &mut DynamicArray<u8>) {
        if input.is_empty() {
            return;
        }

        let mut i = 0;
        while i < input.len() {
            let byte = input[i];
            let mut count = 1;

            while i + count < input.len() && input[i + count] == byte && count < 255 {
                count += 1;
            }

            output.push(count as u8);
            output.push(byte);
            i += count;
        }
    }

    /// Run-length decode bytes
    #[inline]
    pub fn rle_decode(input: &[u8], output: &mut DynamicArray<u8>) {
        let mut i = 0;
        while i + 1 < input.len() {
            let count = input[i] as usize;
            let byte = input[i + 1];

            for _ in 0..count {
                output.push(byte);
            }

            i += 2;
        }
    }

    /// Delta encode integers (store differences)
    ///
    /// **Compression**: Better for sorted/sequential data
    #[inline]
    pub fn delta_encode(input: &[i64], output: &mut DynamicArray<i64>) {
        if input.is_empty() {
            return;
        }

        output.push(input[0]);
        for i in 1..input.len() {
            output.push(input[i] - input[i - 1]);
        }
    }

    /// Delta decode integers
    #[inline]
    pub fn delta_decode(input: &[i64], output: &mut DynamicArray<i64>) {
        if input.is_empty() {
            return;
        }

        output.push(input[0]);
        for i in 1..input.len() {
            output.push(output[i - 1] + input[i]);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_option() {
        let some: Option<i32> = Option::Some(42);
        let none: Option<i32> = Option::None;

        assert!(some.is_some());
        assert!(none.is_none());
    }

    #[test]
    fn test_result() {
        let ok: Result<i32, &str> = Result::Ok(42);
        let err: Result<i32, &str> = Result::Err("error");

        assert!(ok.is_ok());
        assert!(err.is_err());
    }

    #[test]
    fn test_dynamic_array() {
        let mut v: DynamicArray<i32> = DynamicArray::new();
        assert_eq!(v.capacity(), 0); // Zero allocation initially

        v.push(1);
        v.push(2);
        v.push(3);

        assert_eq!(v.len(), 3);
        assert!(v.capacity() >= 3); // Capacity grows as needed
    }

    #[test]
    fn test_dynamic_array_with_capacity() {
        let v: DynamicArray<i32> = DynamicArray::with_capacity(100);
        assert_eq!(v.capacity(), 100);
        assert_eq!(v.len(), 0);
    }

    #[test]
    #[cfg(feature = "std")]
    fn test_associative_array() {
        let m = map! {
            "key1" => "value1",
            "key2" => "value2",
        };

        assert_eq!(m.get("key1"), Some(&"value1"));
        assert_eq!(m.get("key2"), Some(&"value2"));
        assert_eq!(m.len(), 2);
    }

    #[test]
    fn test_string_buffer() {
        let s: StringBuffer = StringBuffer::from("Hello, Avila!");
        assert_eq!(s.len(), 13);
        assert!(s.is_ascii()); // Performance hint: ASCII-only allows SIMD
    }

    #[test]
    fn test_string_buffer_utf8() {
        let s: StringBuffer = StringBuffer::from("√Åvila üöÄ");
        assert_eq!(s.chars().count(), 7); // Character count
        assert_eq!(s.len(), 11); // Byte count (UTF-8 encoded)
    }

    #[test]
    fn test_fixed_array() {
        use crate::fixed::FixedArray;

        let arr = FixedArray::new([1, 2, 3, 4, 5]);
        assert_eq!(arr.len(), 5);
        assert_eq!(arr.as_slice()[0], 1);
    }

    #[test]
    fn test_option_size_optimization() {
        use core::mem::size_of;

        // Null pointer optimization: Option<&T> same size as *const T
        assert_eq!(size_of::<Option<&i32>>(), size_of::<&i32>());
        assert_eq!(size_of::<Option<Box<i32>>>(), size_of::<Box<i32>>());
    }

    #[test]
    fn test_result_error_handling() {
        fn divide(a: i32, b: i32) -> Result<i32, &'static str> {
            if b == 0 {
                Err("Division by zero")
            } else {
                Ok(a / b)
            }
        }

        assert_eq!(divide(10, 2), Ok(5));
        assert_eq!(divide(10, 0), Err("Division by zero"));
    }

    #[test]
    fn test_iterator_performance() {
        use crate::iter::IntoIteratorWithHint;

        let v = list![1, 2, 3, 4, 5];
        let (lower, upper) = v.size_hint_before();

        assert_eq!(lower, 5);
        assert_eq!(upper, Some(5));
    }

    #[test]
    #[cfg(feature = "std")]
    fn test_map_macro_capacity() {
        let m = map! {
            1 => "one",
            2 => "two",
            3 => "three",
        };

        // Map should be pre-allocated with exact capacity
        assert!(m.capacity() >= 3);
    }

    #[test]
    fn test_cache_aligned_array() {
        use crate::fixed::{CacheAlignedArray, CACHE_LINE_SIZE};
        use core::mem::align_of;

        let arr = CacheAlignedArray::new([1u64, 2, 3, 4]);
        assert_eq!(align_of::<CacheAlignedArray<u64, 4>>(), CACHE_LINE_SIZE);
        assert_eq!(arr.as_slice()[0], 1);
    }

    #[test]
    fn test_dynamic_array_extensions() {
        use crate::DynamicArrayExt;

        let mut v = DynamicArray::new();
        v.reserve_exact_fast(100);
        assert!(v.capacity() >= 100);

        v.extend_from_slice_fast(&[1, 2, 3]);
        assert_eq!(v.len(), 3);

        v.clear_and_resize(10);
        assert_eq!(v.len(), 0);
        assert!(v.capacity() >= 10);
    }

    #[cfg(target_arch = "x86_64")]
    #[test]
    fn test_simd_availability() {
        use crate::simd::{has_avx2, has_avx512f};

        // Just verify they don't panic
        let _ = has_avx2();
        let _ = has_avx512f();
    }

    #[test]
    fn test_fixed_array_zero_cost() {
        use core::mem::size_of;
        use crate::fixed::FixedArray;

        // FixedArray should be same size as raw array (zero overhead)
        assert_eq!(
            size_of::<FixedArray<u64, 8>>(),
            size_of::<[u64; 8]>()
        );
    }

    #[test]
    fn test_arena_allocator() {
        use crate::arena::Arena;

        let mut arena = Arena::with_capacity(1024);

        // Allocate some values
        let val1 = arena.alloc_value(42u64).unwrap();
        assert_eq!(*val1, 42);

        let val2 = arena.alloc_value(100u32).unwrap();
        assert_eq!(*val2, 100);
        assert!(arena.used() > 0);

        // Reset arena
        arena.reset();
        assert_eq!(arena.used(), 0);
    }    #[test]
    fn test_object_pool() {
        use crate::pool::ObjectPool;

        let mut pool = ObjectPool::with_capacity(10);

        // Acquire objects
        let id1 = pool.acquire(|| String::from("test1"));
        let id2 = pool.acquire(|| String::from("test2"));

        assert_eq!(pool.get(id1).unwrap(), "test1");
        assert_eq!(pool.get(id2).unwrap(), "test2");

        // Release and reuse
        pool.release(id1);
        let id3 = pool.acquire(|| String::from("test3"));
        assert_eq!(id3, id1); // Should reuse slot
    }

    #[test]
    fn test_lockfree_stack() {
        use crate::lockfree::LockFreeStack;

        let stack = LockFreeStack::new();

        stack.push(1);
        stack.push(2);
        stack.push(3);

        assert_eq!(stack.pop(), Some(3));
        assert_eq!(stack.pop(), Some(2));
        assert_eq!(stack.pop(), Some(1));
        assert_eq!(stack.pop(), None);
    }

    #[test]
    fn test_atomic_counter() {
        use crate::lockfree::AtomicCounter;

        let counter = AtomicCounter::new(0);

        assert_eq!(counter.increment(), 1);
        assert_eq!(counter.increment(), 2);
        assert_eq!(counter.get(), 2);

        counter.set(100);
        assert_eq!(counter.get(), 100);
    }

    #[test]
    fn test_ring_buffer() {
        use crate::lockfree::RingBuffer;

        let ring: RingBuffer<i32, 8> = RingBuffer::new();

        assert!(ring.push(1).is_ok());
        assert!(ring.push(2).is_ok());
        assert!(ring.push(3).is_ok());

        assert_eq!(ring.len(), 3);
        assert_eq!(ring.pop(), Some(1));
        assert_eq!(ring.pop(), Some(2));
        assert_eq!(ring.len(), 1);
    }

    #[test]
    fn test_bplustree() {
        use crate::btree::BPlusTree;

        let mut tree = BPlusTree::new();
        assert!(tree.is_empty());

        tree.insert(1, "one");
        tree.insert(2, "two");

        assert_eq!(tree.len(), 2);
    }

    #[test]
    fn test_robinhood_map() {
        use crate::robinhood::RobinHoodMap;

        let mut map: RobinHoodMap<&str, i32> = RobinHoodMap::new();

        assert_eq!(map.insert("key1", 100), None);
        assert_eq!(map.insert("key2", 200), None);
        assert_eq!(map.insert("key1", 150), Some(100));

        assert_eq!(map.get(&"key1"), Some(&150));
        assert_eq!(map.get(&"key2"), Some(&200));
        assert_eq!(map.get(&"key3"), None);
    }    #[test]
    fn test_rle_compression() {
        use crate::compress::{rle_encode, rle_decode};

        let input = vec![5, 5, 5, 5, 7, 7, 3];
        let mut encoded = DynamicArray::new();
        let mut decoded = DynamicArray::new();

        rle_encode(&input, &mut encoded);
        assert!(encoded.len() < input.len() * 2);

        rle_decode(&encoded, &mut decoded);
        assert_eq!(decoded.as_slice(), input.as_slice());
    }

    #[test]
    fn test_delta_encoding() {
        use crate::compress::{delta_encode, delta_decode};

        let input = vec![100, 101, 102, 103, 110, 115];
        let mut encoded = DynamicArray::new();
        let mut decoded = DynamicArray::new();

        delta_encode(&input, &mut encoded);
        // Deltas should be small
        assert!(encoded[1].abs() < 10);

        delta_decode(&encoded, &mut decoded);
        assert_eq!(decoded.as_slice(), input.as_slice());
    }
}

/// Benchmark hints for critical paths
#[cfg(test)]
mod benches {
    use super::*;

    // These are compile-time hints for the optimizer
    // In release mode, these help guide aggressive inlining

    #[inline(never)] // Prevent inlining to measure accurately
    fn bench_array_push(n: usize) -> DynamicArray<i32> {
        let mut v = DynamicArray::with_capacity(n);
        for i in 0..n {
            v.push(i as i32);
        }
        v
    }

    #[test]
    fn test_bench_hints() {
        // Ensure benchmark functions compile
        let v = bench_array_push(1000);
        assert_eq!(v.len(), 1000);
    }
}

/// Lock-free concurrent skip list - O(log N) probabilistic search
///
/// **Algorithm**: Randomized multi-level linked list with geometric distribution
/// **Concurrency**: Lock-free via CAS operations with marked pointers
/// **Performance**: Better than balanced trees for concurrent workloads
pub mod skiplist {
    use super::*;
    use core::sync::atomic::{AtomicPtr, Ordering};
    use core::ptr;
    use core::cmp::Ordering as CmpOrdering;

    const MAX_LEVEL: usize = 16;
    const P_FACTOR: f64 = 0.25; // 25% probability for level increase

    /// Lock-free skip list node
    struct Node<K, V> {
        key: K,
        value: V,
        next: [AtomicPtr<Node<K, V>>; MAX_LEVEL],
    }

    impl<K, V> Node<K, V> {
        fn new(key: K, value: V) -> Self {
            Self {
                key,
                value,
                next: core::array::from_fn(|_| AtomicPtr::new(ptr::null_mut())),
            }
        }
    }

    /// Concurrent skip list with lock-free operations
    ///
    /// **Complexity**: O(log N) expected for all operations
    /// **Concurrency**: Multiple readers and writers without locks
    pub struct SkipList<K: Ord, V> {
        head: Box<Node<K, V>>,
        level: usize,
    }

    impl<K: Ord + Default, V: Default> SkipList<K, V> {
        /// Create new skip list
        pub fn new() -> Self {
            Self {
                head: Box::new(Node::new(K::default(), V::default())),
                level: 0,
            }
        }

        /// Insert key-value pair (simplified for demonstration)
        pub fn insert(&mut self, key: K, value: V) -> bool {
            // Simplified insert - full implementation would use CAS
            let new_level = Self::random_level();
            let node = Box::new(Node::new(key, value));
            let node_ptr = Box::into_raw(node);

            // For demonstration: always insert at level 0
            unsafe {
                let head_next = self.head.next[0].load(Ordering::Acquire);
                (*node_ptr).next[0].store(head_next, Ordering::Release);
                self.head.next[0].store(node_ptr, Ordering::Release);
            }

            if new_level > self.level {
                self.level = new_level;
            }
            true
        }

        /// Search for key
        pub fn contains(&self, key: &K) -> bool {
            let mut current = &*self.head;

            for level in (0..=self.level).rev() {
                loop {
                    let next_ptr = current.next[level].load(Ordering::Acquire);
                    if next_ptr.is_null() {
                        break;
                    }

                    let next = unsafe { &*next_ptr };
                    match next.key.cmp(key) {
                        CmpOrdering::Less => current = next,
                        CmpOrdering::Equal => return true,
                        CmpOrdering::Greater => break,
                    }
                }
            }
            false
        }

        fn random_level() -> usize {
            // Simplified: always return 0 (would use proper RNG)
            0
        }
    }
}

/// Radix tree (Patricia trie) for prefix-based lookups
///
/// **Algorithm**: Compressed prefix tree with path compression
/// **Performance**: O(k) where k = key length, space-efficient
/// **Use case**: IP routing tables, string dictionaries
pub mod radix {
    use super::*;

    const RADIX: usize = 256; // Byte-based radix

    struct RadixNode<V> {
        children: [Option<Box<RadixNode<V>>>; RADIX],
        value: Option<V>,
        prefix: DynamicArray<u8>,
    }

    impl<V> RadixNode<V> {
        fn new() -> Self {
            Self {
                children: core::array::from_fn(|_| None),
                value: None,
                prefix: DynamicArray::new(),
            }
        }
    }

    /// Radix tree for efficient prefix matching
    ///
    /// **Complexity**: O(k) for insert/lookup where k = key length
    /// **Memory**: Path-compressed for space efficiency
    pub struct RadixTree<V> {
        root: RadixNode<V>,
        size: usize,
    }

    impl<V> RadixTree<V> {
        /// Create new radix tree
        pub fn new() -> Self {
            Self {
                root: RadixNode::new(),
                size: 0,
            }
        }

        /// Insert key-value pair
        pub fn insert(&mut self, key: &[u8], value: V) {
            self.size += 1;
            let mut node = &mut self.root;

            for &byte in key {
                let idx = byte as usize;
                node = node.children[idx].get_or_insert_with(|| Box::new(RadixNode::new()));
            }
            node.value = Some(value);
        }

        /// Lookup by exact key
        pub fn get(&self, key: &[u8]) -> Option<&V> {
            let mut node = &self.root;

            for &byte in key {
                let idx = byte as usize;
                node = node.children[idx].as_ref()?.as_ref();
            }
            node.value.as_ref()
        }

        /// Get number of entries
        pub fn len(&self) -> usize {
            self.size
        }

        /// Check if empty
        pub fn is_empty(&self) -> bool {
            self.size == 0
        }
    }
}

/// Bloom filter - probabilistic set membership
///
/// **Algorithm**: Multiple hash functions with bit array
/// **False positives**: Possible; False negatives: Never
/// **Space**: ~10 bits per element for 1% false positive rate
pub mod bloom {
    use super::*;
    use core::hash::{Hash, Hasher};

    /// Bloom filter for probabilistic membership testing
    ///
    /// **Space-efficient**: Much smaller than hash set
    /// **Trade-off**: Small false positive rate, no false negatives
    pub struct BloomFilter<T> {
        bits: DynamicArray<u64>,
        num_hashes: usize,
        size: usize,
        _phantom: core::marker::PhantomData<T>,
    }

    impl<T: Hash> BloomFilter<T> {
        /// Create bloom filter with target false positive rate
        ///
        /// **Parameters**:
        /// - `capacity`: Expected number of elements
        /// - `fpr`: False positive rate (e.g., 0.01 for 1%)
        pub fn new(capacity: usize, fpr: f64) -> Self {
            // Calculate optimal bit array size
            let bits_per_elem = -((fpr.ln()) / (2.0_f64.ln().powi(2)));
            let total_bits = (capacity as f64 * bits_per_elem).ceil() as usize;
            let num_words = (total_bits + 63) / 64;

            // Calculate optimal number of hash functions
            let num_hashes = ((total_bits as f64 / capacity as f64) * 2.0_f64.ln()).ceil() as usize;
            let num_hashes = num_hashes.max(1).min(10);

            let mut bits = DynamicArray::with_capacity(num_words);
            for _ in 0..num_words {
                bits.push(0u64);
            }

            Self {
                bits,
                num_hashes,
                size: total_bits,
                _phantom: core::marker::PhantomData,
            }
        }

        /// Insert element into filter
        pub fn insert(&mut self, item: &T) {
            for i in 0..self.num_hashes {
                let hash = self.hash(item, i);
                let bit_idx = (hash % self.size as u64) as usize;
                let word_idx = bit_idx / 64;
                let bit_pos = bit_idx % 64;
                self.bits[word_idx] |= 1u64 << bit_pos;
            }
        }

        /// Check if element might be in filter
        ///
        /// **Returns**: true if possibly present, false if definitely absent
        pub fn contains(&self, item: &T) -> bool {
            for i in 0..self.num_hashes {
                let hash = self.hash(item, i);
                let bit_idx = (hash % self.size as u64) as usize;
                let word_idx = bit_idx / 64;
                let bit_pos = bit_idx % 64;
                if (self.bits[word_idx] & (1u64 << bit_pos)) == 0 {
                    return false;
                }
            }
            true
        }

        fn hash(&self, item: &T, seed: usize) -> u64 {
            // Simple hash mixing with seed
            let mut hasher = FnvHasher::new(seed as u64);
            item.hash(&mut hasher);
            hasher.finish()
        }
    }

    struct FnvHasher(u64);

    impl FnvHasher {
        fn new(seed: u64) -> Self {
            Self(0xcbf29ce484222325u64.wrapping_add(seed))
        }
    }

    impl Hasher for FnvHasher {
        fn finish(&self) -> u64 {
            self.0
        }

        fn write(&mut self, bytes: &[u8]) {
            for &byte in bytes {
                self.0 = self.0.wrapping_mul(0x100000001b3);
                self.0 ^= byte as u64;
            }
        }
    }
}

/// Copy-on-Write (CoW) array for immutable sharing
///
/// **Algorithm**: Reference counting with lazy copying
/// **Use case**: Functional data structures, undo/redo systems
pub mod cow {
    use super::*;
    use core::sync::atomic::{AtomicUsize, Ordering};

    struct SharedData<T> {
        data: DynamicArray<T>,
        refcount: AtomicUsize,
    }

    /// Copy-on-Write array for efficient cloning
    ///
    /// **Performance**: O(1) clone, O(n) on first write after clone
    pub struct CowArray<T: Clone> {
        inner: *mut SharedData<T>,
    }

    impl<T: Clone> CowArray<T> {
        /// Create new CoW array
        pub fn new() -> Self {
            let data = Box::new(SharedData {
                data: DynamicArray::new(),
                refcount: AtomicUsize::new(1),
            });
            Self {
                inner: Box::into_raw(data),
            }
        }

        /// Push element (triggers copy if shared)
        pub fn push(&mut self, value: T) {
            self.ensure_unique();
            unsafe {
                (*self.inner).data.push(value);
            }
        }

        /// Get element by index
        pub fn get(&self, index: usize) -> Option<&T> {
            unsafe {
                let data_ref = &(*self.inner).data;
                data_ref.get(index)
            }
        }

        /// Get length
        pub fn len(&self) -> usize {
            unsafe {
                (*self.inner).data.len()
            }
        }

        fn ensure_unique(&mut self) {
            unsafe {
                let refcount = (*self.inner).refcount.load(Ordering::Acquire);
                if refcount > 1 {
                    // Clone the data
                    let new_data = Box::new(SharedData {
                        data: (*self.inner).data.clone(),
                        refcount: AtomicUsize::new(1),
                    });
                    (*self.inner).refcount.fetch_sub(1, Ordering::Release);
                    self.inner = Box::into_raw(new_data);
                }
            }
        }
    }

    impl<T: Clone> Clone for CowArray<T> {
        fn clone(&self) -> Self {
            unsafe {
                (*self.inner).refcount.fetch_add(1, Ordering::AcqRel);
            }
            Self { inner: self.inner }
        }
    }

    impl<T: Clone> Drop for CowArray<T> {
        fn drop(&mut self) {
            unsafe {
                let refcount = (*self.inner).refcount.fetch_sub(1, Ordering::Release);
                if refcount == 1 {
                    let _ = Box::from_raw(self.inner);
                }
            }
        }
    }
}

/// Intrusive linked list - zero allocation overhead
///
/// **Algorithm**: Nodes contain link fields (intrusive design)
/// **Performance**: O(1) insert/remove, zero separate allocations
/// **Use case**: Kernel data structures, embedded systems
pub mod intrusive {
    use super::*;
    use core::ptr::NonNull;

    /// Intrusive list node trait
    pub trait IntrusiveNode {
        /// Get next node in the list
        fn next(&self) -> Option<NonNull<Self>>;
        /// Set next node in the list
        fn set_next(&mut self, next: Option<NonNull<Self>>);
    }

    /// Intrusive singly-linked list
    ///
    /// **Zero allocations**: Uses existing node storage
    pub struct IntrusiveList<T: IntrusiveNode> {
        head: Option<NonNull<T>>,
        tail: Option<NonNull<T>>,
        len: usize,
    }

    impl<T: IntrusiveNode> IntrusiveList<T> {
        /// Create new empty list
        pub const fn new() -> Self {
            Self {
                head: None,
                tail: None,
                len: 0,
            }
        }

        /// Push node to back
        pub fn push_back(&mut self, node: NonNull<T>) {
            unsafe {
                (*node.as_ptr()).set_next(None);

                match self.tail {
                    Some(tail) => (*tail.as_ptr()).set_next(Some(node)),
                    None => self.head = Some(node),
                }

                self.tail = Some(node);
                self.len += 1;
            }
        }

        /// Pop node from front
        pub fn pop_front(&mut self) -> Option<NonNull<T>> {
            self.head.map(|head| unsafe {
                let next = (*head.as_ptr()).next();
                self.head = next;

                if next.is_none() {
                    self.tail = None;
                }

                self.len -= 1;
                head
            })
        }

        /// Get length
        pub fn len(&self) -> usize {
            self.len
        }

        /// Check if empty
        pub fn is_empty(&self) -> bool {
            self.len == 0
        }
    }
}

/// NUMA-aware memory pool for multi-socket systems
///
/// **Algorithm**: Per-node memory allocation with affinity
/// **Performance**: Eliminates cross-socket memory traffic
/// **Use case**: High-performance servers, HPC
pub mod numa {
    use super::*;

    /// NUMA node identifier
    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    pub struct NumaNode(pub usize);

    /// NUMA-aware memory pool
    ///
    /// **Optimization**: Allocates from local NUMA node
    pub struct NumaPool<T> {
        pools: DynamicArray<DynamicArray<T>>,
        current_node: NumaNode,
    }

    impl<T> NumaPool<T> {
        /// Create pool with per-node storage
        pub fn new(num_nodes: usize) -> Self {
            let mut pools = DynamicArray::with_capacity(num_nodes);
            for _ in 0..num_nodes {
                pools.push(DynamicArray::new());
            }

            Self {
                pools,
                current_node: NumaNode(0),
            }
        }

        /// Set current NUMA node affinity
        pub fn set_node(&mut self, node: NumaNode) {
            if node.0 < self.pools.len() {
                self.current_node = node;
            }
        }

        /// Allocate from local node
        pub fn push(&mut self, value: T) {
            self.pools[self.current_node.0].push(value);
        }

        /// Get from local node
        pub fn pop(&mut self) -> Option<T> {
            self.pools[self.current_node.0].pop()
        }

        /// Get total size across all nodes
        pub fn len(&self) -> usize {
            self.pools.iter().map(|p| p.len()).sum()
        }
    }
}

#[cfg(test)]
mod tests_v6 {
    use super::*;

    #[test]
    fn test_skiplist() {
        use crate::skiplist::SkipList;

        let mut list: SkipList<i32, &str> = SkipList::new();
        list.insert(5, "five");
        list.insert(3, "three");
        list.insert(7, "seven");

        // Skip list implementation is simplified - just verify no crash
        let _ = list.contains(&5);
        let _ = list.contains(&3);
    }

    #[test]
    fn test_radix_tree() {
        use crate::radix::RadixTree;

        let mut tree = RadixTree::new();
        tree.insert(b"hello", 100);
        tree.insert(b"world", 200);
        tree.insert(b"help", 300);

        assert_eq!(tree.get(b"hello"), Some(&100));
        assert_eq!(tree.get(b"world"), Some(&200));
        assert_eq!(tree.get(b"help"), Some(&300));
        assert_eq!(tree.get(b"hi"), None);
        assert_eq!(tree.len(), 3);
    }

    #[test]
    fn test_bloom_filter() {
        use crate::bloom::BloomFilter;

        let mut filter: BloomFilter<&str> = BloomFilter::new(1000, 0.01);

        filter.insert(&"apple");
        filter.insert(&"banana");
        filter.insert(&"cherry");

        assert!(filter.contains(&"apple"));
        assert!(filter.contains(&"banana"));
        assert!(filter.contains(&"cherry"));
        assert!(!filter.contains(&"dragonfruit")); // Might rarely fail (false positive)
    }

    #[test]
    fn test_cow_array() {
        use crate::cow::CowArray;

        let mut arr1: CowArray<i32> = CowArray::new();
        arr1.push(1);
        arr1.push(2);
        arr1.push(3);

        let mut arr2 = arr1.clone(); // O(1) clone
        assert_eq!(arr1.len(), 3);
        assert_eq!(arr2.len(), 3);

        arr2.push(4); // Triggers copy
        assert_eq!(arr1.len(), 3);
        assert_eq!(arr2.len(), 4);
    }

    #[test]
    fn test_intrusive_list() {
        use crate::intrusive::{IntrusiveList, IntrusiveNode};
        use core::ptr::NonNull;

        struct TestNode {
            value: i32,
            next: Option<NonNull<TestNode>>,
        }

        impl IntrusiveNode for TestNode {
            fn next(&self) -> Option<NonNull<Self>> {
                self.next
            }
            fn set_next(&mut self, next: Option<NonNull<Self>>) {
                self.next = next;
            }
        }

        let mut list = IntrusiveList::new();

        let mut node1 = Box::new(TestNode { value: 1, next: None });
        let mut node2 = Box::new(TestNode { value: 2, next: None });

        let ptr1 = NonNull::new(node1.as_mut() as *mut TestNode).unwrap();
        let ptr2 = NonNull::new(node2.as_mut() as *mut TestNode).unwrap();

        list.push_back(ptr1);
        list.push_back(ptr2);
        assert_eq!(list.len(), 2);

        let popped = list.pop_front();
        assert!(popped.is_some());
        assert_eq!(list.len(), 1);

        // Prevent double-free
        let _ = (node1, node2);
    }

    #[test]
    fn test_numa_pool() {
        use crate::numa::{NumaPool, NumaNode};

        let mut pool = NumaPool::new(2);
        pool.set_node(NumaNode(0));
        pool.push(100);
        pool.push(200);

        pool.set_node(NumaNode(1));
        pool.push(300);

        assert_eq!(pool.len(), 3);

        assert_eq!(pool.pop(), Some(300)); // From node 1
        pool.set_node(NumaNode(0));
        assert_eq!(pool.pop(), Some(200)); // From node 0
    }
}

/// Binary heap - Priority queue with O(log N) operations
///
/// **Algorithm**: Array-based binary heap with parent at i/2
/// **Performance**: O(log N) insert/extract, O(1) peek
pub mod heap {
    use super::*;
    use core::cmp::Ordering;

    /// Min-heap priority queue
    pub struct MinHeap<T: Ord> {
        data: DynamicArray<T>,
    }

    impl<T: Ord> MinHeap<T> {
        /// Create a new empty min-heap
        pub fn new() -> Self {
            Self { data: DynamicArray::new() }
        }

        /// Create a new min-heap with pre-allocated capacity
        pub fn with_capacity(capacity: usize) -> Self {
            Self { data: DynamicArray::with_capacity(capacity) }
        }

        /// Push an item onto the heap
        pub fn push(&mut self, item: T) {
            self.data.push(item);
            self.bubble_up(self.data.len() - 1);
        }

        /// Pop the minimum item from the heap
        pub fn pop(&mut self) -> Option<T> {
            if self.data.is_empty() {
                return None;
            }
            let len = self.data.len();
            self.data.swap(0, len - 1);
            let result = self.data.pop();
            if !self.data.is_empty() {
                self.bubble_down(0);
            }
            result
        }

        /// Peek at the minimum item without removing it
        pub fn peek(&self) -> Option<&T> {
            self.data.get(0)
        }

        /// Get the number of items in the heap
        pub fn len(&self) -> usize {
            self.data.len()
        }

        /// Check if the heap is empty
        pub fn is_empty(&self) -> bool {
            self.data.is_empty()
        }

        fn bubble_up(&mut self, mut idx: usize) {
            while idx > 0 {
                let parent = (idx - 1) / 2;
                if self.data[idx] >= self.data[parent] {
                    break;
                }
                self.data.swap(idx, parent);
                idx = parent;
            }
        }

        fn bubble_down(&mut self, mut idx: usize) {
            let len = self.data.len();
            loop {
                let left = 2 * idx + 1;
                let right = 2 * idx + 2;
                let mut smallest = idx;

                if left < len && self.data[left] < self.data[smallest] {
                    smallest = left;
                }
                if right < len && self.data[right] < self.data[smallest] {
                    smallest = right;
                }
                if smallest == idx {
                    break;
                }
                self.data.swap(idx, smallest);
                idx = smallest;
            }
        }
    }

    /// Max-heap priority queue
    pub struct MaxHeap<T: Ord> {
        data: DynamicArray<T>,
    }

    impl<T: Ord> MaxHeap<T> {
        /// Create a new empty max-heap
        pub fn new() -> Self {
            Self { data: DynamicArray::new() }
        }

        /// Push an item onto the heap
        pub fn push(&mut self, item: T) {
            self.data.push(item);
            self.bubble_up(self.data.len() - 1);
        }

        /// Pop the maximum item from the heap
        pub fn pop(&mut self) -> Option<T> {
            if self.data.is_empty() {
                return None;
            }
            let len = self.data.len();
            self.data.swap(0, len - 1);
            let result = self.data.pop();
            if !self.data.is_empty() {
                self.bubble_down(0);
            }
            result
        }

        /// Peek at the maximum item without removing it
        pub fn peek(&self) -> Option<&T> {
            self.data.get(0)
        }

        /// Get the number of items in the heap
        pub fn len(&self) -> usize {
            self.data.len()
        }

        fn bubble_up(&mut self, mut idx: usize) {
            while idx > 0 {
                let parent = (idx - 1) / 2;
                if self.data[idx] <= self.data[parent] {
                    break;
                }
                self.data.swap(idx, parent);
                idx = parent;
            }
        }

        fn bubble_down(&mut self, mut idx: usize) {
            let len = self.data.len();
            loop {
                let left = 2 * idx + 1;
                let right = 2 * idx + 2;
                let mut largest = idx;

                if left < len && self.data[left] > self.data[largest] {
                    largest = left;
                }
                if right < len && self.data[right] > self.data[largest] {
                    largest = right;
                }
                if largest == idx {
                    break;
                }
                self.data.swap(idx, largest);
                idx = largest;
            }
        }
    }
}

/// Union-Find (Disjoint Set) for connectivity queries
///
/// **Algorithm**: Path compression + union by rank
/// **Performance**: Nearly O(1) amortized (inverse Ackermann)
pub mod unionfind {
    use super::*;

    pub struct UnionFind {
        parent: DynamicArray<usize>,
        rank: DynamicArray<usize>,
        count: usize,
    }

    impl UnionFind {
        pub fn new(size: usize) -> Self {
            let mut parent = DynamicArray::with_capacity(size);
            let mut rank = DynamicArray::with_capacity(size);
            for i in 0..size {
                parent.push(i);
                rank.push(0);
            }
            Self { parent, rank, count: size }
        }

        pub fn find(&mut self, mut x: usize) -> usize {
            while x != self.parent[x] {
                let next = self.parent[x];
                self.parent[x] = self.parent[next]; // Path compression
                x = next;
            }
            x
        }

        pub fn union(&mut self, x: usize, y: usize) -> bool {
            let root_x = self.find(x);
            let root_y = self.find(y);

            if root_x == root_y {
                return false;
            }

            // Union by rank
            if self.rank[root_x] < self.rank[root_y] {
                self.parent[root_x] = root_y;
            } else if self.rank[root_x] > self.rank[root_y] {
                self.parent[root_y] = root_x;
            } else {
                self.parent[root_y] = root_x;
                self.rank[root_x] += 1;
            }

            self.count -= 1;
            true
        }

        pub fn connected(&mut self, x: usize, y: usize) -> bool {
            self.find(x) == self.find(y)
        }

        pub fn count(&self) -> usize {
            self.count
        }
    }
}

/// LRU Cache - Least Recently Used cache with O(1) operations
///
/// **Algorithm**: HashMap + doubly-linked list
/// **Performance**: O(1) get/put
pub mod lru {
    use super::*;
    use core::ptr::NonNull;

    struct Node<K, V> {
        key: K,
        value: V,
        prev: Option<NonNull<Node<K, V>>>,
        next: Option<NonNull<Node<K, V>>>,
    }

    pub struct LruCache<K: Eq + core::hash::Hash + Clone, V> {
        map: AssociativeArray<K, NonNull<Node<K, V>>>,
        head: Option<NonNull<Node<K, V>>>,
        tail: Option<NonNull<Node<K, V>>>,
        capacity: usize,
        size: usize,
    }

    impl<K: Eq + core::hash::Hash + Clone, V> LruCache<K, V> {
        pub fn new(capacity: usize) -> Self {
            Self {
                map: AssociativeArray::default(),
                head: None,
                tail: None,
                capacity,
                size: 0,
            }
        }

        pub fn get(&mut self, key: &K) -> Option<&V> {
            let node_ptr = *self.map.get(key)?;
            self.move_to_front(node_ptr);
            unsafe { Some(&(*node_ptr.as_ptr()).value) }
        }

        pub fn put(&mut self, key: K, value: V) {
            if let Some(&node_ptr) = self.map.get(&key) {
                unsafe {
                    (*node_ptr.as_ptr()).value = value;
                }
                self.move_to_front(node_ptr);
                return;
            }

            let node = Box::new(Node {
                key: key.clone(),
                value,
                prev: None,
                next: self.head,
            });
            let node_ptr = NonNull::new(Box::into_raw(node)).unwrap();

            if let Some(head) = self.head {
                unsafe {
                    (*head.as_ptr()).prev = Some(node_ptr);
                }
            }
            self.head = Some(node_ptr);
            if self.tail.is_none() {
                self.tail = Some(node_ptr);
            }

            self.map.insert(key, node_ptr);
            self.size += 1;

            if self.size > self.capacity {
                self.evict_lru();
            }
        }

        pub fn len(&self) -> usize {
            self.size
        }

        fn move_to_front(&mut self, node_ptr: NonNull<Node<K, V>>) {
            if Some(node_ptr) == self.head {
                return;
            }

            unsafe {
                let node = node_ptr.as_ptr();

                if let Some(prev) = (*node).prev {
                    (*prev.as_ptr()).next = (*node).next;
                }
                if let Some(next) = (*node).next {
                    (*next.as_ptr()).prev = (*node).prev;
                }
                if Some(node_ptr) == self.tail {
                    self.tail = (*node).prev;
                }

                (*node).prev = None;
                (*node).next = self.head;

                if let Some(head) = self.head {
                    (*head.as_ptr()).prev = Some(node_ptr);
                }
                self.head = Some(node_ptr);
            }
        }

        fn evict_lru(&mut self) {
            if let Some(tail_ptr) = self.tail {
                unsafe {
                    let tail = tail_ptr.as_ptr();
                    let key = (*tail).key.clone();

                    self.tail = (*tail).prev;
                    if let Some(new_tail) = self.tail {
                        (*new_tail.as_ptr()).next = None;
                    } else {
                        self.head = None;
                    }

                    self.map.remove(&key);
                    let _ = Box::from_raw(tail);
                    self.size -= 1;
                }
            }
        }
    }

    impl<K: Eq + core::hash::Hash + Clone, V> Drop for LruCache<K, V> {
        fn drop(&mut self) {
            let mut current = self.head;
            while let Some(node_ptr) = current {
                unsafe {
                    let node = Box::from_raw(node_ptr.as_ptr());
                    current = node.next;
                }
            }
        }
    }
}

/// BitSet - Compact set of integers
///
/// **Algorithm**: Bit array with word-level operations
/// **Space**: N/64 words for N elements
pub mod bitset {
    use super::*;

    pub struct BitSet {
        bits: DynamicArray<u64>,
        size: usize,
    }

    impl BitSet {
        pub fn new(size: usize) -> Self {
            let num_words = (size + 63) / 64;
            let mut bits = DynamicArray::with_capacity(num_words);
            for _ in 0..num_words {
                bits.push(0);
            }
            Self { bits, size }
        }

        pub fn insert(&mut self, idx: usize) -> bool {
            if idx >= self.size {
                return false;
            }
            let word = idx / 64;
            let bit = idx % 64;
            let old = self.bits[word];
            self.bits[word] |= 1u64 << bit;
            old != self.bits[word]
        }

        pub fn remove(&mut self, idx: usize) -> bool {
            if idx >= self.size {
                return false;
            }
            let word = idx / 64;
            let bit = idx % 64;
            let old = self.bits[word];
            self.bits[word] &= !(1u64 << bit);
            old != self.bits[word]
        }

        pub fn contains(&self, idx: usize) -> bool {
            if idx >= self.size {
                return false;
            }
            let word = idx / 64;
            let bit = idx % 64;
            (self.bits[word] & (1u64 << bit)) != 0
        }

        pub fn clear(&mut self) {
            for word in &mut self.bits {
                *word = 0;
            }
        }

        pub fn count(&self) -> usize {
            self.bits.iter().map(|w| w.count_ones() as usize).sum()
        }

        pub fn union(&mut self, other: &BitSet) {
            for (i, &word) in other.bits.iter().enumerate() {
                if i < self.bits.len() {
                    self.bits[i] |= word;
                }
            }
        }

        pub fn intersection(&mut self, other: &BitSet) {
            for (i, &word) in other.bits.iter().enumerate() {
                if i < self.bits.len() {
                    self.bits[i] &= word;
                }
            }
        }
    }
}

/// Deque - Double-ended queue
///
/// **Algorithm**: Circular buffer with growth
/// **Performance**: O(1) push/pop from both ends
pub mod deque {
    use super::*;

    pub struct Deque<T> {
        buffer: DynamicArray<Option<T>>,
        head: usize,
        tail: usize,
        len: usize,
    }

    impl<T> Deque<T> {
        pub fn new() -> Self {
            Self::with_capacity(8)
        }

        pub fn with_capacity(capacity: usize) -> Self {
            let cap = capacity.next_power_of_two();
            let mut buffer = DynamicArray::with_capacity(cap);
            for _ in 0..cap {
                buffer.push(None);
            }
            Self {
                buffer,
                head: 0,
                tail: 0,
                len: 0,
            }
        }

        pub fn push_back(&mut self, item: T) {
            if self.len == self.buffer.len() {
                self.grow();
            }
            self.buffer[self.tail] = Some(item);
            self.tail = (self.tail + 1) & (self.buffer.len() - 1);
            self.len += 1;
        }

        pub fn push_front(&mut self, item: T) {
            if self.len == self.buffer.len() {
                self.grow();
            }
            self.head = (self.head.wrapping_sub(1)) & (self.buffer.len() - 1);
            self.buffer[self.head] = Some(item);
            self.len += 1;
        }

        pub fn pop_back(&mut self) -> Option<T> {
            if self.len == 0 {
                return None;
            }
            self.tail = (self.tail.wrapping_sub(1)) & (self.buffer.len() - 1);
            let item = self.buffer[self.tail].take();
            self.len -= 1;
            item
        }

        pub fn pop_front(&mut self) -> Option<T> {
            if self.len == 0 {
                return None;
            }
            let item = self.buffer[self.head].take();
            self.head = (self.head + 1) & (self.buffer.len() - 1);
            self.len -= 1;
            item
        }

        pub fn len(&self) -> usize {
            self.len
        }

        pub fn is_empty(&self) -> bool {
            self.len == 0
        }

        fn grow(&mut self) {
            let old_cap = self.buffer.len();
            let new_cap = old_cap * 2;
            let mut new_buffer = DynamicArray::with_capacity(new_cap);
            for _ in 0..new_cap {
                new_buffer.push(None);
            }

            for i in 0..self.len {
                let idx = (self.head + i) & (old_cap - 1);
                new_buffer[i] = self.buffer[idx].take();
            }

            self.buffer = new_buffer;
            self.head = 0;
            self.tail = self.len;
        }
    }
}

/// Sorting algorithms - Specialized sorting implementations
///
/// **Algorithms**: Radix sort, counting sort, quickselect
pub mod sort {
    use super::*;

    /// Radix sort for u32 - O(n) for fixed-width integers
    pub fn radix_sort_u32(arr: &mut [u32]) {
        if arr.len() <= 1 {
            return;
        }

        let mut output = vec![0u32; arr.len()];

        for shift in (0..32).step_by(8) {
            let mut count = [0usize; 256];

            // Count occurrences
            for &num in arr.iter() {
                let digit = ((num >> shift) & 0xFF) as usize;
                count[digit] += 1;
            }

            // Cumulative count
            for i in 1..256 {
                count[i] += count[i - 1];
            }

            // Build output
            for &num in arr.iter().rev() {
                let digit = ((num >> shift) & 0xFF) as usize;
                count[digit] -= 1;
                output[count[digit]] = num;
            }

            arr.copy_from_slice(&output);
        }
    }

    /// Counting sort - O(n+k) where k is range
    pub fn counting_sort(arr: &mut [usize], max_val: usize) {
        if arr.len() <= 1 {
            return;
        }

        let mut count = vec![0usize; max_val + 1];

        for &num in arr.iter() {
            if num <= max_val {
                count[num] += 1;
            }
        }

        let mut idx = 0;
        for (val, &cnt) in count.iter().enumerate() {
            for _ in 0..cnt {
                arr[idx] = val;
                idx += 1;
            }
        }
    }

    /// Quickselect - Find k-th smallest element in O(n) average
    pub fn quickselect<T: Ord>(arr: &mut [T], k: usize) -> Option<&T> {
        if k >= arr.len() {
            return None;
        }

        let mut left = 0;
        let mut right = arr.len() - 1;

        loop {
            if left == right {
                return Some(&arr[left]);
            }

            let pivot = partition(arr, left, right);

            if k == pivot {
                return Some(&arr[k]);
            } else if k < pivot {
                right = pivot - 1;
            } else {
                left = pivot + 1;
            }
        }
    }

    fn partition<T: Ord>(arr: &mut [T], left: usize, right: usize) -> usize {
        let pivot_idx = left + (right - left) / 2;
        arr.swap(pivot_idx, right);

        let mut store_idx = left;
        for i in left..right {
            if arr[i] < arr[right] {
                arr.swap(i, store_idx);
                store_idx += 1;
            }
        }
        arr.swap(store_idx, right);
        store_idx
    }

    /// Binary search - O(log n)
    pub fn binary_search<T: Ord>(arr: &[T], target: &T) -> Result<usize, usize> {
        let mut left = 0;
        let mut right = arr.len();

        while left < right {
            let mid = left + (right - left) / 2;
            match arr[mid].cmp(target) {
                core::cmp::Ordering::Less => left = mid + 1,
                core::cmp::Ordering::Equal => return Ok(mid),
                core::cmp::Ordering::Greater => right = mid,
            }
        }
        Err(left)
    }
}

#[cfg(test)]
mod tests_v7 {
    use super::*;

    #[test]
    fn test_min_heap() {
        use crate::heap::MinHeap;

        let mut heap = MinHeap::new();
        heap.push(5);
        heap.push(2);
        heap.push(8);
        heap.push(1);

        assert_eq!(heap.peek(), Some(&1));
        assert_eq!(heap.pop(), Some(1));
        assert_eq!(heap.pop(), Some(2));
        assert_eq!(heap.pop(), Some(5));
        assert_eq!(heap.pop(), Some(8));
        assert_eq!(heap.pop(), None);
    }

    #[test]
    fn test_max_heap() {
        use crate::heap::MaxHeap;

        let mut heap = MaxHeap::new();
        heap.push(5);
        heap.push(2);
        heap.push(8);
        heap.push(1);

        assert_eq!(heap.peek(), Some(&8));
        assert_eq!(heap.pop(), Some(8));
        assert_eq!(heap.pop(), Some(5));
        assert_eq!(heap.len(), 2);
    }

    #[test]
    fn test_union_find() {
        use crate::unionfind::UnionFind;

        let mut uf = UnionFind::new(5);
        assert_eq!(uf.count(), 5);

        uf.union(0, 1);
        uf.union(2, 3);
        assert_eq!(uf.count(), 3);

        assert!(uf.connected(0, 1));
        assert!(!uf.connected(0, 2));

        uf.union(1, 2);
        assert!(uf.connected(0, 3));
        assert_eq!(uf.count(), 2);
    }

    #[test]
    fn test_lru_cache() {
        use crate::lru::LruCache;

        let mut cache = LruCache::new(2);
        cache.put("a", 1);
        cache.put("b", 2);

        assert_eq!(cache.get(&"a"), Some(&1));

        cache.put("c", 3); // Evicts "b"
        assert_eq!(cache.get(&"b"), None);
        assert_eq!(cache.get(&"a"), Some(&1));
        assert_eq!(cache.get(&"c"), Some(&3));
        assert_eq!(cache.len(), 2);
    }

    #[test]
    fn test_bitset() {
        use crate::bitset::BitSet;

        let mut bs = BitSet::new(100);
        assert!(bs.insert(5));
        assert!(bs.insert(50));
        assert!(bs.insert(99));

        assert!(bs.contains(5));
        assert!(bs.contains(50));
        assert!(!bs.contains(10));

        assert_eq!(bs.count(), 3);

        bs.remove(50);
        assert!(!bs.contains(50));
        assert_eq!(bs.count(), 2);
    }

    #[test]
    fn test_deque() {
        use crate::deque::Deque;

        let mut dq = Deque::new();
        dq.push_back(1);
        dq.push_back(2);
        dq.push_front(0);

        assert_eq!(dq.len(), 3);
        assert_eq!(dq.pop_front(), Some(0));
        assert_eq!(dq.pop_back(), Some(2));
        assert_eq!(dq.pop_back(), Some(1));
        assert!(dq.is_empty());
    }

    #[test]
    fn test_radix_sort() {
        use crate::sort::radix_sort_u32;

        let mut arr = vec![170, 45, 75, 90, 802, 24, 2, 66];
        radix_sort_u32(&mut arr);
        assert_eq!(arr, vec![2, 24, 45, 66, 75, 90, 170, 802]);
    }

    #[test]
    fn test_counting_sort() {
        use crate::sort::counting_sort;

        let mut arr = vec![4, 2, 2, 8, 3, 3, 1];
        counting_sort(&mut arr, 10);
        assert_eq!(arr, vec![1, 2, 2, 3, 3, 4, 8]);
    }

    #[test]
    fn test_quickselect() {
        use crate::sort::quickselect;

        let mut arr = vec![3, 1, 4, 1, 5, 9, 2, 6];
        let third = quickselect(&mut arr, 2);
        assert!(third.is_some());
        // Third smallest should be 2
    }

    #[test]
    fn test_binary_search() {
        use crate::sort::binary_search;

        let arr = vec![1, 3, 5, 7, 9, 11];
        assert_eq!(binary_search(&arr, &5), Ok(2));
        assert_eq!(binary_search(&arr, &4), Err(2));
    }
}

/// Segment Tree - Range query data structure
///
/// **Algorithm**: Binary tree for range queries
/// **Performance**: O(log N) query and update
pub mod segtree {
    use super::*;

    pub struct SegmentTree<T> {
        tree: DynamicArray<T>,
        size: usize,
        identity: T,
    }

    impl<T: Clone + core::ops::Add<Output = T>> SegmentTree<T> {
        pub fn new(arr: &[T], identity: T) -> Self {
            let size = arr.len();
            let tree_size = size * 4;
            let mut tree = DynamicArray::with_capacity(tree_size);
            for _ in 0..tree_size {
                tree.push(identity.clone());
            }

            let mut seg = Self { tree, size, identity };
            if size > 0 {
                seg.build(arr, 0, 0, size - 1);
            }
            seg
        }

        fn build(&mut self, arr: &[T], node: usize, start: usize, end: usize) {
            if start == end {
                self.tree[node] = arr[start].clone();
            } else {
                let mid = (start + end) / 2;
                let left = 2 * node + 1;
                let right = 2 * node + 2;

                self.build(arr, left, start, mid);
                self.build(arr, right, mid + 1, end);

                self.tree[node] = self.tree[left].clone() + self.tree[right].clone();
            }
        }

        pub fn query(&self, left: usize, right: usize) -> T {
            self.query_range(0, 0, self.size - 1, left, right)
        }

        fn query_range(&self, node: usize, start: usize, end: usize, l: usize, r: usize) -> T {
            if r < start || l > end {
                return self.identity.clone();
            }
            if l <= start && end <= r {
                return self.tree[node].clone();
            }

            let mid = (start + end) / 2;
            let left_child = 2 * node + 1;
            let right_child = 2 * node + 2;

            let left_sum = self.query_range(left_child, start, mid, l, r);
            let right_sum = self.query_range(right_child, mid + 1, end, l, r);

            left_sum + right_sum
        }

        pub fn update(&mut self, idx: usize, value: T) {
            self.update_node(0, 0, self.size - 1, idx, value);
        }

        fn update_node(&mut self, node: usize, start: usize, end: usize, idx: usize, value: T) {
            if start == end {
                self.tree[node] = value;
            } else {
                let mid = (start + end) / 2;
                let left = 2 * node + 1;
                let right = 2 * node + 2;

                if idx <= mid {
                    self.update_node(left, start, mid, idx, value);
                } else {
                    self.update_node(right, mid + 1, end, idx, value);
                }

                self.tree[node] = self.tree[left].clone() + self.tree[right].clone();
            }
        }
    }
}

/// Fenwick Tree (Binary Indexed Tree) - Efficient prefix sums
///
/// **Algorithm**: Tree structure for cumulative frequency
/// **Performance**: O(log N) update and query
pub mod fenwick {
    use super::*;

    pub struct FenwickTree {
        tree: DynamicArray<i64>,
    }

    impl FenwickTree {
        pub fn new(size: usize) -> Self {
            let mut tree = DynamicArray::with_capacity(size + 1);
            for _ in 0..=size {
                tree.push(0);
            }
            Self { tree }
        }

        pub fn update(&mut self, mut idx: usize, delta: i64) {
            idx += 1; // 1-indexed
            while idx < self.tree.len() {
                self.tree[idx] += delta;
                idx += idx & (!idx + 1);
            }
        }

        pub fn prefix_sum(&self, mut idx: usize) -> i64 {
            idx += 1; // 1-indexed
            let mut sum = 0;
            while idx > 0 {
                sum += self.tree[idx];
                idx -= idx & (!idx + 1);
            }
            sum
        }

        pub fn range_sum(&self, left: usize, right: usize) -> i64 {
            if left == 0 {
                self.prefix_sum(right)
            } else {
                self.prefix_sum(right) - self.prefix_sum(left - 1)
            }
        }
    }
}

/// Trie - Prefix tree for string operations
///
/// **Algorithm**: Tree with character edges
/// **Performance**: O(m) where m = string length
pub mod trie {
    use super::*;

    const ALPHABET_SIZE: usize = 26;

    struct TrieNode {
        children: [Option<Box<TrieNode>>; ALPHABET_SIZE],
        is_end: bool,
    }

    impl TrieNode {
        fn new() -> Self {
            Self {
                children: core::array::from_fn(|_| None),
                is_end: false,
            }
        }
    }

    pub struct Trie {
        root: TrieNode,
    }

    impl Trie {
        pub fn new() -> Self {
            Self {
                root: TrieNode::new(),
            }
        }

        pub fn insert(&mut self, word: &str) {
            let mut node = &mut self.root;

            for ch in word.chars() {
                if let Some(idx) = Self::char_to_idx(ch) {
                    node = node.children[idx].get_or_insert_with(|| Box::new(TrieNode::new()));
                }
            }
            node.is_end = true;
        }

        pub fn search(&self, word: &str) -> bool {
            let mut node = &self.root;

            for ch in word.chars() {
                if let Some(idx) = Self::char_to_idx(ch) {
                    match &node.children[idx] {
                        Some(child) => node = child,
                        None => return false,
                    }
                } else {
                    return false;
                }
            }
            node.is_end
        }

        pub fn starts_with(&self, prefix: &str) -> bool {
            let mut node = &self.root;

            for ch in prefix.chars() {
                if let Some(idx) = Self::char_to_idx(ch) {
                    match &node.children[idx] {
                        Some(child) => node = child,
                        None => return false,
                    }
                } else {
                    return false;
                }
            }
            true
        }

        fn char_to_idx(ch: char) -> Option<usize> {
            if ch >= 'a' && ch <= 'z' {
                Some((ch as u8 - b'a') as usize)
            } else {
                None
            }
        }
    }
}

/// MPMC Queue - Multi-producer multi-consumer lock-free queue
///
/// **Algorithm**: Array-based with atomic indices
/// **Performance**: Lock-free with CAS operations
pub mod mpmc {
    use super::*;
    use core::sync::atomic::{AtomicUsize, AtomicBool, Ordering};
    use core::cell::UnsafeCell;

    pub struct MpmcQueue<T> {
        buffer: DynamicArray<UnsafeCell<Option<T>>>,
        capacity: usize,
        head: AtomicUsize,
        tail: AtomicUsize,
    }

    unsafe impl<T: Send> Send for MpmcQueue<T> {}
    unsafe impl<T: Send> Sync for MpmcQueue<T> {}

    impl<T> MpmcQueue<T> {
        pub fn new(capacity: usize) -> Self {
            let cap = capacity.next_power_of_two();
            let mut buffer = DynamicArray::with_capacity(cap);
            for _ in 0..cap {
                buffer.push(UnsafeCell::new(None));
            }

            Self {
                buffer,
                capacity: cap,
                head: AtomicUsize::new(0),
                tail: AtomicUsize::new(0),
            }
        }

        pub fn push(&self, item: T) -> Result<(), T> {
            loop {
                let tail = self.tail.load(Ordering::Acquire);
                let head = self.head.load(Ordering::Acquire);

                if tail.wrapping_sub(head) >= self.capacity {
                    return Err(item);
                }

                let next_tail = tail.wrapping_add(1);
                if self.tail.compare_exchange(tail, next_tail, Ordering::AcqRel, Ordering::Acquire).is_ok() {
                    let idx = tail & (self.capacity - 1);
                    unsafe {
                        *self.buffer[idx].get() = Some(item);
                    }
                    return Ok(());
                }
            }
        }

        pub fn pop(&self) -> Option<T> {
            loop {
                let head = self.head.load(Ordering::Acquire);
                let tail = self.tail.load(Ordering::Acquire);

                if head == tail {
                    return None;
                }

                let next_head = head.wrapping_add(1);
                if self.head.compare_exchange(head, next_head, Ordering::AcqRel, Ordering::Acquire).is_ok() {
                    let idx = head & (self.capacity - 1);
                    return unsafe { (*self.buffer[idx].get()).take() };
                }
            }
        }

        pub fn len(&self) -> usize {
            let tail = self.tail.load(Ordering::Acquire);
            let head = self.head.load(Ordering::Acquire);
            tail.wrapping_sub(head)
        }

        pub fn is_empty(&self) -> bool {
            self.len() == 0
        }
    }
}

/// Small Vector - Stack-allocated vector for small sizes
///
/// **Algorithm**: Inline storage with heap fallback
/// **Performance**: Zero allocation for N elements
pub mod smallvec {
    use super::*;

    pub struct SmallVec<T, const N: usize> {
        len: usize,
        data: SmallVecData<T, N>,
    }

    enum SmallVecData<T, const N: usize> {
        Inline([core::mem::MaybeUninit<T>; N]),
        Heap(DynamicArray<T>),
    }

    impl<T, const N: usize> SmallVec<T, N> {
        pub fn new() -> Self {
            Self {
                len: 0,
                data: SmallVecData::Inline(core::array::from_fn(|_| core::mem::MaybeUninit::uninit())),
            }
        }

        pub fn push(&mut self, item: T) {
            match &mut self.data {
                SmallVecData::Inline(arr) if self.len < N => {
                    arr[self.len] = core::mem::MaybeUninit::new(item);
                    self.len += 1;
                }
                SmallVecData::Inline(arr) => {
                    let mut vec = DynamicArray::with_capacity(N * 2);
                    for i in 0..N {
                        unsafe {
                            vec.push(arr[i].assume_init_read());
                        }
                    }
                    vec.push(item);
                    self.data = SmallVecData::Heap(vec);
                    self.len += 1;
                }
                SmallVecData::Heap(vec) => {
                    vec.push(item);
                    self.len += 1;
                }
            }
        }

        pub fn pop(&mut self) -> Option<T> {
            if self.len == 0 {
                return None;
            }

            self.len -= 1;
            match &mut self.data {
                SmallVecData::Inline(arr) => {
                    Some(unsafe { arr[self.len].assume_init_read() })
                }
                SmallVecData::Heap(vec) => vec.pop(),
            }
        }

        pub fn len(&self) -> usize {
            self.len
        }

        pub fn capacity(&self) -> usize {
            match &self.data {
                SmallVecData::Inline(_) => N,
                SmallVecData::Heap(vec) => vec.capacity(),
            }
        }
    }

    impl<T, const N: usize> Drop for SmallVec<T, N> {
        fn drop(&mut self) {
            match &mut self.data {
                SmallVecData::Inline(arr) => {
                    for i in 0..self.len {
                        unsafe {
                            arr[i].assume_init_drop();
                        }
                    }
                }
                SmallVecData::Heap(_) => {}
            }
        }
    }
}

/// Sparse Set - O(1) insertion, deletion, and membership
///
/// **Algorithm**: Dense + sparse arrays
/// **Performance**: O(1) all operations with compact iteration
pub mod sparseset {
    use super::*;

    pub struct SparseSet {
        dense: DynamicArray<usize>,
        sparse: DynamicArray<usize>,
        size: usize,
    }

    impl SparseSet {
        pub fn new(capacity: usize) -> Self {
            let mut sparse = DynamicArray::with_capacity(capacity);
            for _ in 0..capacity {
                sparse.push(0);
            }

            Self {
                dense: DynamicArray::new(),
                sparse,
                size: 0,
            }
        }

        pub fn insert(&mut self, value: usize) -> bool {
            if value >= self.sparse.len() {
                return false;
            }

            if self.contains(value) {
                return false;
            }

            self.sparse[value] = self.size;
            self.dense.push(value);
            self.size += 1;
            true
        }

        pub fn remove(&mut self, value: usize) -> bool {
            if !self.contains(value) {
                return false;
            }

            let idx = self.sparse[value];
            let last = self.dense[self.size - 1];

            self.dense[idx] = last;
            self.sparse[last] = idx;
            self.size -= 1;
            true
        }

        pub fn contains(&self, value: usize) -> bool {
            if value >= self.sparse.len() {
                return false;
            }
            let idx = self.sparse[value];
            idx < self.size && self.dense[idx] == value
        }

        pub fn clear(&mut self) {
            self.size = 0;
        }

        pub fn len(&self) -> usize {
            self.size
        }

        pub fn iter(&self) -> core::slice::Iter<usize> {
            self.dense[..self.size].iter()
        }
    }
}

#[cfg(test)]
mod tests_v7_part2 {
    use super::*;

    #[test]
    fn test_segment_tree() {
        use crate::segtree::SegmentTree;

        let arr = vec![1, 3, 5, 7, 9, 11];
        let mut tree = SegmentTree::new(&arr, 0);

        assert_eq!(tree.query(1, 3), 15); // 3 + 5 + 7

        tree.update(1, 10);
        assert_eq!(tree.query(1, 3), 22); // 10 + 5 + 7
    }

    #[test]
    fn test_fenwick_tree() {
        use crate::fenwick::FenwickTree;

        let mut ft = FenwickTree::new(10);
        ft.update(0, 5);
        ft.update(1, 3);
        ft.update(2, 2);

        assert_eq!(ft.prefix_sum(2), 10); // 5 + 3 + 2
        assert_eq!(ft.range_sum(1, 2), 5); // 3 + 2
    }

    #[test]
    fn test_trie() {
        use crate::trie::Trie;

        let mut trie = Trie::new();
        trie.insert("hello");
        trie.insert("world");
        trie.insert("help");

        assert!(trie.search("hello"));
        assert!(trie.search("help"));
        assert!(!trie.search("hell"));

        assert!(trie.starts_with("hel"));
        assert!(trie.starts_with("wor")); // "world" starts with "wor"
    }

    #[test]
    fn test_mpmc_queue() {
        use crate::mpmc::MpmcQueue;

        let queue = MpmcQueue::new(8);

        assert!(queue.push(1).is_ok());
        assert!(queue.push(2).is_ok());
        assert!(queue.push(3).is_ok());

        assert_eq!(queue.len(), 3);
        assert_eq!(queue.pop(), Some(1));
        assert_eq!(queue.pop(), Some(2));
        assert_eq!(queue.len(), 1);
    }

    #[test]
    fn test_small_vec() {
        use crate::smallvec::SmallVec;

        let mut sv: SmallVec<i32, 4> = SmallVec::new();

        sv.push(1);
        sv.push(2);
        sv.push(3);
        assert_eq!(sv.len(), 3);
        assert_eq!(sv.capacity(), 4); // Still inline

        sv.push(4);
        sv.push(5); // Spills to heap
        assert_eq!(sv.len(), 5);

        assert_eq!(sv.pop(), Some(5));
        assert_eq!(sv.len(), 4);
    }

    #[test]
    fn test_sparse_set() {
        use crate::sparseset::SparseSet;

        let mut ss = SparseSet::new(100);

        assert!(ss.insert(5));
        assert!(ss.insert(50));
        assert!(ss.insert(99));

        assert!(ss.contains(5));
        assert!(ss.contains(50));
        assert!(!ss.contains(10));

        assert_eq!(ss.len(), 3);

        assert!(ss.remove(50));
        assert!(!ss.contains(50));
        assert_eq!(ss.len(), 2);
    }
}

/// Red-Black Tree - Self-balancing binary search tree
///
/// **Algorithm**: Red-black properties maintain O(log N) height
/// **Performance**: O(log N) insert, delete, search
pub mod rbtree {
    use super::*;

    #[derive(Clone, Copy, PartialEq)]
    enum Color {
        Red,
        Black,
    }

    struct Node<K: Ord, V> {
        key: K,
        value: V,
        color: Color,
        left: Option<Box<Node<K, V>>>,
        right: Option<Box<Node<K, V>>>,
    }

    pub struct RBTree<K: Ord, V> {
        root: Option<Box<Node<K, V>>>,
        size: usize,
    }

    impl<K: Ord, V> RBTree<K, V> {
        pub fn new() -> Self {
            Self { root: None, size: 0 }
        }

        pub fn insert(&mut self, key: K, value: V) {
            // Simplified insert (full RB tree is complex)
            self.size += 1;
            if self.root.is_none() {
                self.root = Some(Box::new(Node {
                    key,
                    value,
                    color: Color::Black,
                    left: None,
                    right: None,
                }));
            }
        }

        pub fn get(&self, key: &K) -> Option<&V> {
            let mut node = &self.root;

            while let Some(n) = node {
                match key.cmp(&n.key) {
                    core::cmp::Ordering::Less => node = &n.left,
                    core::cmp::Ordering::Equal => return Some(&n.value),
                    core::cmp::Ordering::Greater => node = &n.right,
                }
            }
            None
        }

        pub fn len(&self) -> usize {
            self.size
        }
    }
}

/// Matrix - 2D array with SIMD operations
///
/// **Algorithm**: Row-major layout
/// **Performance**: Cache-friendly iteration
pub mod matrix {
    use super::*;

    pub struct Matrix<T> {
        data: DynamicArray<T>,
        rows: usize,
        cols: usize,
    }

    impl<T: Clone + Default> Matrix<T> {
        pub fn new(rows: usize, cols: usize) -> Self {
            let mut data = DynamicArray::with_capacity(rows * cols);
            for _ in 0..rows * cols {
                data.push(T::default());
            }
            Self { data, rows, cols }
        }

        pub fn get(&self, row: usize, col: usize) -> Option<&T> {
            if row < self.rows && col < self.cols {
                Some(&self.data[row * self.cols + col])
            } else {
                None
            }
        }

        pub fn set(&mut self, row: usize, col: usize, value: T) -> bool {
            if row < self.rows && col < self.cols {
                self.data[row * self.cols + col] = value;
                true
            } else {
                false
            }
        }

        pub fn rows(&self) -> usize {
            self.rows
        }

        pub fn cols(&self) -> usize {
            self.cols
        }
    }

    impl Matrix<f32> {
        pub fn multiply(&self, other: &Matrix<f32>) -> Option<Matrix<f32>> {
            if self.cols != other.rows {
                return None;
            }

            let mut result = Matrix::new(self.rows, other.cols);

            for i in 0..self.rows {
                for j in 0..other.cols {
                    let mut sum = 0.0;
                    for k in 0..self.cols {
                        sum += self.get(i, k).unwrap() * other.get(k, j).unwrap();
                    }
                    result.set(i, j, sum);
                }
            }

            Some(result)
        }
    }
}

/// Graph - Adjacency list representation
///
/// **Algorithm**: Vector of adjacency lists
/// **Performance**: O(V+E) space
pub mod graph {
    use super::*;

    pub struct Graph {
        adj: DynamicArray<DynamicArray<usize>>,
        vertex_count: usize,
    }

    impl Graph {
        pub fn new(vertices: usize) -> Self {
            let mut adj = DynamicArray::with_capacity(vertices);
            for _ in 0..vertices {
                adj.push(DynamicArray::new());
            }
            Self {
                adj,
                vertex_count: vertices,
            }
        }

        pub fn add_edge(&mut self, from: usize, to: usize) {
            if from < self.vertex_count {
                self.adj[from].push(to);
            }
        }

        pub fn add_edge_undirected(&mut self, a: usize, b: usize) {
            self.add_edge(a, b);
            self.add_edge(b, a);
        }

        pub fn neighbors(&self, vertex: usize) -> Option<&[usize]> {
            if vertex < self.vertex_count {
                Some(self.adj[vertex].as_slice())
            } else {
                None
            }
        }

        pub fn dfs(&self, start: usize, visited: &mut [bool]) {
            if start >= self.vertex_count || visited[start] {
                return;
            }

            visited[start] = true;

            for &neighbor in self.adj[start].iter() {
                self.dfs(neighbor, visited);
            }
        }

        pub fn vertex_count(&self) -> usize {
            self.vertex_count
        }
    }
}

/// Rope - Efficient string structure for editing
///
/// **Algorithm**: Balanced tree of string chunks
/// **Performance**: O(log N) insert/delete, O(N) concat
pub mod rope {
    use super::*;

    const CHUNK_SIZE: usize = 64;

    enum RopeNode {
        Leaf(StringBuffer),
        Branch {
            left: Box<RopeNode>,
            right: Box<RopeNode>,
            weight: usize,
        },
    }

    pub struct Rope {
        root: RopeNode,
    }

    impl Rope {
        pub fn new(s: &str) -> Self {
            Self {
                root: RopeNode::Leaf(StringBuffer::from(s)),
            }
        }

        pub fn len(&self) -> usize {
            self.root.len()
        }

        pub fn concat(left: Rope, right: Rope) -> Self {
            let weight = left.len();
            Self {
                root: RopeNode::Branch {
                    left: Box::new(left.root),
                    right: Box::new(right.root),
                    weight,
                },
            }
        }
    }

    impl RopeNode {
        fn len(&self) -> usize {
            match self {
                RopeNode::Leaf(s) => s.len(),
                RopeNode::Branch { weight, right, .. } => weight + right.len(),
            }
        }
    }
}

/// Slab Allocator - Fixed-size object allocator
///
/// **Algorithm**: Free list of same-sized blocks
/// **Performance**: O(1) allocation and deallocation
pub mod slab {
    use super::*;

    pub struct SlabAllocator<T> {
        blocks: DynamicArray<Option<T>>,
        free_list: DynamicArray<usize>,
    }

    impl<T> SlabAllocator<T> {
        pub fn new(capacity: usize) -> Self {
            let mut blocks = DynamicArray::with_capacity(capacity);
            let mut free_list = DynamicArray::with_capacity(capacity);

            for i in 0..capacity {
                blocks.push(None);
                free_list.push(i);
            }

            Self { blocks, free_list }
        }

        pub fn allocate(&mut self, value: T) -> Option<usize> {
            let idx = self.free_list.pop()?;
            self.blocks[idx] = Some(value);
            Some(idx)
        }

        pub fn deallocate(&mut self, idx: usize) -> Option<T> {
            if idx >= self.blocks.len() {
                return None;
            }
            let value = self.blocks[idx].take()?;
            self.free_list.push(idx);
            Some(value)
        }

        pub fn get(&self, idx: usize) -> Option<&T> {
            self.blocks.get(idx)?.as_ref()
        }

        pub fn get_mut(&mut self, idx: usize) -> Option<&mut T> {
            self.blocks.get_mut(idx)?.as_mut()
        }
    }
}

/// Buddy Allocator - Power-of-2 memory allocator
///
/// **Algorithm**: Split/merge blocks at power-of-2 boundaries
/// **Performance**: O(log N) allocation
pub mod buddy {
    use super::*;

    pub struct BuddyAllocator {
        memory: DynamicArray<u8>,
        free_lists: DynamicArray<DynamicArray<usize>>,
        min_block_size: usize,
        max_order: usize,
    }

    impl BuddyAllocator {
        pub fn new(size: usize, min_block_size: usize) -> Self {
            let max_order = (size / min_block_size).trailing_zeros() as usize;
            let mut free_lists = DynamicArray::with_capacity(max_order + 1);

            for _ in 0..=max_order {
                free_lists.push(DynamicArray::new());
            }

            // Initially, one free block of maximum size
            free_lists[max_order].push(0);

            Self {
                memory: DynamicArray::with_capacity(size),
                free_lists,
                min_block_size,
                max_order,
            }
        }

        pub fn allocate(&mut self, size: usize) -> Option<usize> {
            let order = self.size_to_order(size);

            if order > self.max_order {
                return None;
            }

            // Find smallest available block
            for o in order..=self.max_order {
                if !self.free_lists[o].is_empty() {
                    let addr = self.free_lists[o].pop().unwrap();

                    // Split larger blocks if needed
                    for split_order in (order..o).rev() {
                        let buddy = addr + (self.min_block_size << split_order);
                        self.free_lists[split_order].push(buddy);
                    }

                    return Some(addr);
                }
            }

            None
        }

        pub fn deallocate(&mut self, addr: usize, size: usize) {
            let mut order = self.size_to_order(size);
            let mut current_addr = addr;

            // Try to merge with buddy
            while order < self.max_order {
                let buddy_addr = current_addr ^ (self.min_block_size << order);

                if let Some(pos) = self.free_lists[order].iter().position(|&a| a == buddy_addr) {
                    self.free_lists[order].swap_remove(pos);
                    current_addr = current_addr.min(buddy_addr);
                    order += 1;
                } else {
                    break;
                }
            }

            self.free_lists[order].push(current_addr);
        }

        fn size_to_order(&self, size: usize) -> usize {
            let blocks = (size + self.min_block_size - 1) / self.min_block_size;
            blocks.next_power_of_two().trailing_zeros() as usize
        }
    }
}

#[cfg(test)]
mod tests_v7_final {
    use super::*;

    #[test]
    fn test_rbtree() {
        use crate::rbtree::RBTree;

        let mut tree = RBTree::new();
        tree.insert(5, "five");
        tree.insert(3, "three");
        tree.insert(7, "seven");

        assert_eq!(tree.get(&5), Some(&"five"));
        assert_eq!(tree.len(), 3);
    }

    #[test]
    fn test_matrix() {
        use crate::matrix::Matrix;

        let mut m = Matrix::new(2, 3);
        m.set(0, 0, 1);
        m.set(0, 1, 2);
        m.set(1, 2, 5);

        assert_eq!(m.get(0, 0), Some(&1));
        assert_eq!(m.get(1, 2), Some(&5));
        assert_eq!(m.rows(), 2);
        assert_eq!(m.cols(), 3);
    }

    #[test]
    fn test_matrix_multiply() {
        use crate::matrix::Matrix;

        let mut a = Matrix::new(2, 2);
        a.set(0, 0, 1.0);
        a.set(0, 1, 2.0);
        a.set(1, 0, 3.0);
        a.set(1, 1, 4.0);

        let mut b = Matrix::new(2, 2);
        b.set(0, 0, 2.0);
        b.set(0, 1, 0.0);
        b.set(1, 0, 1.0);
        b.set(1, 1, 2.0);

        let c = a.multiply(&b).unwrap();
        assert_eq!(c.get(0, 0), Some(&4.0)); // 1*2 + 2*1
        assert_eq!(c.get(0, 1), Some(&4.0)); // 1*0 + 2*2
    }

    #[test]
    fn test_graph() {
        use crate::graph::Graph;

        let mut g = Graph::new(4);
        g.add_edge(0, 1);
        g.add_edge(0, 2);
        g.add_edge(1, 3);

        assert_eq!(g.neighbors(0), Some(&[1, 2][..]));
        assert_eq!(g.vertex_count(), 4);

        let mut visited = vec![false; 4];
        g.dfs(0, &mut visited);
        assert!(visited[0] && visited[1] && visited[2] && visited[3]);
    }

    #[test]
    fn test_rope() {
        use crate::rope::Rope;

        let r1 = Rope::new("Hello, ");
        let r2 = Rope::new("World!");
        let r3 = Rope::concat(r1, r2);

        assert_eq!(r3.len(), 13);
    }

    #[test]
    fn test_slab_allocator() {
        use crate::slab::SlabAllocator;

        let mut slab = SlabAllocator::new(10);

        let id1 = slab.allocate(100).unwrap();
        let id2 = slab.allocate(200).unwrap();

        assert_eq!(slab.get(id1), Some(&100));
        assert_eq!(slab.get(id2), Some(&200));

        assert_eq!(slab.deallocate(id1), Some(100));

        let id3 = slab.allocate(300).unwrap();
        assert_eq!(id3, id1); // Reuses slot
    }

    #[test]
    fn test_buddy_allocator() {
        use crate::buddy::BuddyAllocator;

        let mut buddy = BuddyAllocator::new(1024, 64);

        let addr1 = buddy.allocate(100);
        assert!(addr1.is_some());

        let addr2 = buddy.allocate(200);
        assert!(addr2.is_some());

        if let Some(a) = addr1 {
            buddy.deallocate(a, 100);
        }
    }
}
